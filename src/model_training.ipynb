{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db43fcdf",
   "metadata": {},
   "source": [
    "# ASL Recognition: End-to-End Workflow\n",
    "\n",
    "This notebook consolidates the entire workflow for preprocessing, model training, and evaluation for American Sign Language (ASL) recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a040ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b7725",
   "metadata": {},
   "source": [
    "# Preprocessing: Load and Normalize Keypoints\n",
    "\n",
    "We will load the preprocessed keypoints from the JSON files and normalize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4673b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(r'f:\\Uni_Stuff\\6th_Sem\\DL\\Proj\\video-asl-recognition\\pose_estimation\\data\\keypoints')\n",
    "\n",
    "# Load and normalize keypoints\n",
    "def load_keypoints(data_dir):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for label_dir in tqdm(data_dir.iterdir(), desc=\"Loading labels\"):\n",
    "        if not label_dir.is_dir():\n",
    "            continue\n",
    "        for json_file in label_dir.glob('*.json'):\n",
    "            with open(json_file, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                keypoints = content['keypoints']\n",
    "                label = content['label']\n",
    "                # Normalize keypoints\n",
    "                keypoints = np.array(keypoints)\n",
    "                keypoints[:, :, :2] = keypoints[:, :, :2] / 1.0  # Assuming already normalized\n",
    "                data.append(keypoints)\n",
    "                labels.append(label)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Load data\n",
    "data, labels = load_keypoints(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60166d",
   "metadata": {},
   "source": [
    "# Dataset Preparation: Train-Test Split\n",
    "\n",
    "Split the dataset into training, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data, labels, test_size=0.3, stratify=labels, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e866d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class ASLDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = ASLDataset(X_train, y_train)\n",
    "val_dataset = ASLDataset(X_val, y_val)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb276e",
   "metadata": {},
   "source": [
    "# Model Definition: BiLSTM for Temporal Data\n",
    "\n",
    "Define a BiLSTM model for ASL recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95231d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define the model with Batch Normalization\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.batch_norm1(x.transpose(1, 2)).transpose(1, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.batch_norm2(x.transpose(1, 2)).transpose(1, 2)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "input_size = X_train.shape[2]\n",
    "hidden_size = 128\n",
    "output_size = len(np.unique(labels))\n",
    "model = BiLSTM(input_size, hidden_size, output_size)\n",
    "print(model)\n",
    "# Move model to selected device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9840ccc",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Tune Hyperparams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning Code\n",
    "\n",
    "# Define a small grid of hyperparameters to search over\n",
    "learning_rates = [0.001, 0.0005]\n",
    "hidden_sizes = [64, 128]\n",
    "num_epochs = 10  # Fewer epochs for tuning\n",
    "tuning_patience = 3\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_params = {}\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for hidden in hidden_sizes:\n",
    "        print(f\"\\nTesting configuration: lr={lr}, hidden_size={hidden}\")\n",
    "        # Create a new model instance with the current hyperparameters\n",
    "        model_tuned = BiLSTM(input_size, hidden, output_size).to(device)\n",
    "        optimizer_tuned = optim.Adam(model_tuned.parameters(), lr=lr)\n",
    "        criterion_tuned = nn.CrossEntropyLoss()\n",
    "        \n",
    "        curr_best_val_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model_tuned.train()\n",
    "            train_loss = 0\n",
    "            for batch in train_loader:\n",
    "                inputs, targets = batch\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer_tuned.zero_grad()\n",
    "                outputs = model_tuned(inputs)\n",
    "                loss = criterion_tuned(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer_tuned.step()\n",
    "                train_loss += loss.item()\n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            model_tuned.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    inputs, targets = batch\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model_tuned(inputs)\n",
    "                    loss = criterion_tuned(outputs, targets)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            if val_loss < curr_best_val_loss:\n",
    "                curr_best_val_loss = val_loss\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= tuning_patience:\n",
    "                    print(\"Early stopping for this configuration.\")\n",
    "                    break\n",
    "        \n",
    "        results[(lr, hidden)] = curr_best_val_loss\n",
    "        if curr_best_val_loss < best_val_loss:\n",
    "            best_val_loss = curr_best_val_loss\n",
    "            best_params = {'learning_rate': lr, 'hidden_size': hidden}\n",
    "\n",
    "print(\"\\nHyperparameter Tuning Results:\")\n",
    "for (lr, hidden), loss in results.items():\n",
    "    print(f\"lr: {lr}, hidden_size: {hidden} => Val Loss: {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nBest hyperparameters: {best_params} with Val Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17410aea",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Train the model on the training set and validate on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b730f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Use best hyperparameters from tuning\n",
    "# Get the best hyperparameters\n",
    "best_lr = best_params['learning_rate']\n",
    "best_hidden_size = best_params['hidden_size']\n",
    "\n",
    "# Recreate model with best hyperparameters\n",
    "model = BiLSTM(input_size, best_hidden_size, output_size).to(device)\n",
    "print(f\"Training with best hyperparameters: lr={best_lr}, hidden_size={best_hidden_size}\")\n",
    "\n",
    "# Define the loss function and optimizer with best learning rate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Training with Early Stopping\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "patience = 5\n",
    "early_stop_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    # Save the model if validation loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"Model checkpoint saved!\")\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"Early stopping counter: {early_stop_counter}/{patience}\")\n",
    "    if early_stop_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Loss Curves\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curves\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7bf6d",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Evaluate the model on the test set and display metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_dataset = ASLDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "test_loss /= len(test_loader)\n",
    "accuracy = correct / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the model for transfer learning\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'input_size': input_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'output_size': output_size\n",
    "}, \"transfer_learning_model.pth\")\n",
    "print(\"Model saved for transfer learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90776ea",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Test the model on new data or perform real-time inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb18cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "model.eval()\n",
    "sample = torch.tensor(X_test[0:1], dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    prediction = model(sample)\n",
    "    predicted_label = torch.argmax(prediction, dim=1).item()\n",
    "print(f\"Predicted label: {predicted_label}, True label: {y_test[0]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
