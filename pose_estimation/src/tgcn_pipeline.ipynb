{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282aa4ff",
   "metadata": {},
   "source": [
    "# ASL Recognition with TGCN - Complete Pipeline\n",
    "\n",
    "## üöÄ Improved TGCN with Face Landmarks (553 Nodes)\n",
    "\n",
    "This notebook implements a state-of-the-art ASL recognition system using:\n",
    "\n",
    "- **553 keypoints**: 33 pose + 42 hands + 478 face landmarks\n",
    "- **Advanced preprocessing**: Spatial anchoring, temporal smoothing, interpolation\n",
    "- **Improved graph connectivity**: Anatomical + functional relationships\n",
    "- **Data augmentation**: Spatial and temporal transformations\n",
    "- **WLASL-100 subset**: Focus on quality over quantity\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "- **Input**: MediaPipe keypoint sequences (seq_len, 553, 3)\n",
    "- **Graph**: Enhanced connectivity with face-hand relationships\n",
    "- **Model**: ST-GCN with temporal convolutions\n",
    "- **Target**: 87.60% accuracy on WLASL-100 (literature benchmark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e965f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch Geometric 2.6.1 loaded\n",
      "üöÄ Improved normalization and preprocessing module loaded!\n",
      "üìö Based on successful TGCN implementations achieving 87.60% on WLASL-100\n",
      "‚ú® Features: spatial anchoring, temporal smoothing, improved graph connectivity\n",
      "üìä Supporting 553-node architecture: 33 pose + 42 hands + 478 face landmarks\n",
      "üéØ All libraries loaded successfully!\n",
      "PyTorch version: 2.7.0+cu118\n",
      "CUDA available: True\n",
      "Using device: cuda\n",
      "üöÄ Improved normalization and preprocessing module loaded!\n",
      "üìö Based on successful TGCN implementations achieving 87.60% on WLASL-100\n",
      "‚ú® Features: spatial anchoring, temporal smoothing, improved graph connectivity\n",
      "üìä Supporting 553-node architecture: 33 pose + 42 hands + 478 face landmarks\n",
      "üéØ All libraries loaded successfully!\n",
      "PyTorch version: 2.7.0+cu118\n",
      "CUDA available: True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# PyTorch Geometric for GCN\n",
    "try:\n",
    "    import torch_geometric\n",
    "    from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "    from torch_geometric.data import Data, Batch\n",
    "    print(f\"‚úÖ PyTorch Geometric {torch_geometric.__version__} loaded\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå PyTorch Geometric not found. Install with: pip install torch-geometric\")\n",
    "    raise\n",
    "\n",
    "# Data handling and visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our improved normalization module\n",
    "from normalization import (\n",
    "    ImprovedPoseNormalizer,\n",
    "    create_improved_pose_dataset_class,\n",
    "    create_improved_graph_connectivity,\n",
    "    apply_spatial_augmentation,\n",
    "    apply_temporal_augmentation\n",
    ")\n",
    "\n",
    "print(\"üéØ All libraries loaded successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc298d4",
   "metadata": {},
   "source": [
    "## üìä Configuration and Data Paths\n",
    "\n",
    "Set up all paths and hyperparameters for the training pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f7b605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration:\n",
      "  max_seq_len: 50\n",
      "  num_nodes: 553\n",
      "  num_features: 3\n",
      "  max_classes: 100\n",
      "  test_size: 0.2\n",
      "  batch_size: 16\n",
      "  gcn_hidden: 256\n",
      "  temporal_kernel: 9\n",
      "  dropout: 0.3\n",
      "  num_gcn_layers: 3\n",
      "  num_epochs: 100\n",
      "  learning_rate: 0.001\n",
      "  weight_decay: 0.0001\n",
      "  patience: 15\n",
      "  min_lr: 1e-06\n",
      "  use_augmentation: True\n",
      "  aug_probability: 0.3\n",
      "  spatial_aug_strength: 0.1\n",
      "  temporal_aug_strength: 0.2\n",
      "\n",
      "üìÅ Data directory: f:\\Uni_Stuff\\6th_Sem\\DL\\Proj\\video-asl-recognition\\pose_estimation\\data\\keypoints\n",
      "üíæ Checkpoint directory: f:\\Uni_Stuff\\6th_Sem\\DL\\Proj\\video-asl-recognition\\pose_estimation\\src\\checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "DATA_DIR = r'f:\\Uni_Stuff\\6th_Sem\\DL\\Proj\\video-asl-recognition\\pose_estimation\\data\\keypoints'\n",
    "CHECKPOINT_DIR = r'f:\\Uni_Stuff\\6th_Sem\\DL\\Proj\\video-asl-recognition\\pose_estimation\\src\\checkpoints'\n",
    "MODEL_SAVE_PATH = os.path.join(CHECKPOINT_DIR, 'best_tgcn_face_model.pth')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Model hyperparameters\n",
    "CONFIG = {\n",
    "    # Data parameters\n",
    "    'max_seq_len': 50,          # Maximum sequence length\n",
    "    'num_nodes': 553,           # 33 pose + 42 hands + 478 face\n",
    "    'num_features': 3,          # x, y, z coordinates\n",
    "    'max_classes': 100,         # Use WLASL-100 subset\n",
    "    'test_size': 0.2,           # Train/test split ratio\n",
    "    'batch_size': 16,           # Batch size (reduced for 553 nodes)\n",
    "    \n",
    "    # Model architecture\n",
    "    'gcn_hidden': 256,          # GCN hidden dimensions\n",
    "    'temporal_kernel': 9,       # Temporal convolution kernel size\n",
    "    'dropout': 0.3,             # Dropout rate\n",
    "    'num_gcn_layers': 3,        # Number of GCN layers\n",
    "    \n",
    "    # Training parameters\n",
    "    'num_epochs': 100,          # Maximum epochs\n",
    "    'learning_rate': 0.001,     # Initial learning rate\n",
    "    'weight_decay': 1e-4,       # L2 regularization\n",
    "    'patience': 15,             # Early stopping patience\n",
    "    'min_lr': 1e-6,             # Minimum learning rate\n",
    "    \n",
    "    # Data augmentation\n",
    "    'use_augmentation': True,   # Enable data augmentation\n",
    "    'aug_probability': 0.3,     # Probability of applying augmentation\n",
    "    'spatial_aug_strength': 0.1, # Spatial augmentation strength\n",
    "    'temporal_aug_strength': 0.2, # Temporal augmentation strength\n",
    "}\n",
    "\n",
    "print(\"üìã Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüìÅ Data directory: {DATA_DIR}\")\n",
    "print(f\"üíæ Checkpoint directory: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e4f9d",
   "metadata": {},
   "source": [
    "## üîç Data Exploration and Validation\n",
    "\n",
    "Explore the keypoint data to understand the dataset structure and validate the 553-node architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(data_dir):\n",
    "    \"\"\"Explore the keypoint dataset structure and statistics\"\"\"\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"‚ùå Data directory not found: {data_dir}\")\n",
    "        print(\"Please run the keypoint extraction first with pose_estimation_mediapipe.py\")\n",
    "        return None\n",
    "    \n",
    "    # Find all word directories\n",
    "    word_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    word_dirs = sorted(word_dirs)\n",
    "    \n",
    "    print(f\"üìä Dataset Statistics:\")\n",
    "    print(f\"  Total classes found: {len(word_dirs)}\")\n",
    "    \n",
    "    # Analyze sample distribution\n",
    "    class_stats = []\n",
    "    total_files = 0\n",
    "    sample_shapes = []\n",
    "    \n",
    "    for word in word_dirs[:20]:  # Check first 20 classes\n",
    "        word_dir = os.path.join(data_dir, word)\n",
    "        npz_files = glob.glob(os.path.join(word_dir, \"*.npz\"))\n",
    "        total_files += len(npz_files)\n",
    "        \n",
    "        # Check sample file shape\n",
    "        if npz_files:\n",
    "            try:\n",
    "                sample_data = np.load(npz_files[0])\n",
    "                if 'nodes' in sample_data:\n",
    "                    shape = sample_data['nodes'].shape\n",
    "                    sample_shapes.append(shape)\n",
    "                    print(f\"  {word}: {len(npz_files)} files, shape: {shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {word}: {len(npz_files)} files, error reading: {e}\")\n",
    "        \n",
    "        class_stats.append((word, len(npz_files)))\n",
    "    \n",
    "    print(f\"\\nüìà Sample distribution (top 20):\")\n",
    "    class_stats.sort(key=lambda x: x[1], reverse=True)\n",
    "    for word, count in class_stats[:10]:\n",
    "        print(f\"  {word}: {count} samples\")\n",
    "    \n",
    "    # Validate node architecture\n",
    "    if sample_shapes:\n",
    "        most_common_shape = max(set(sample_shapes), key=sample_shapes.count)\n",
    "        print(f\"\\nüèóÔ∏è Architecture validation:\")\n",
    "        print(f\"  Most common shape: {most_common_shape}\")\n",
    "        print(f\"  Expected nodes: {CONFIG['num_nodes']} (33 pose + 42 hands + 478 face)\")\n",
    "        \n",
    "        if most_common_shape[1] == CONFIG['num_nodes']:\n",
    "            print(f\"  ‚úÖ Architecture matches! Found {most_common_shape[1]} nodes\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Architecture mismatch! Found {most_common_shape[1]}, expected {CONFIG['num_nodes']}\")\n",
    "            if most_common_shape[1] == 75:\n",
    "                print(f\"  üìù Data contains only pose+hands (75 nodes). Need to re-run extraction with face landmarks.\")\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Explore the dataset\n",
    "data_ready = explore_dataset(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e89d53",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Dataset Class and Data Loading\n",
    "\n",
    "Create the dataset class with improved normalization and load the data for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da62b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the improved dataset class\n",
    "ImprovedPoseSequenceDataset = create_improved_pose_dataset_class()\n",
    "\n",
    "# Initialize datasets\n",
    "if data_ready:\n",
    "    print(\"üîÑ Creating datasets...\")\n",
    "    \n",
    "    # Training dataset\n",
    "    train_dataset = ImprovedPoseSequenceDataset(\n",
    "        data_dir=DATA_DIR,\n",
    "        max_seq_len=CONFIG['max_seq_len'],\n",
    "        split='train',\n",
    "        test_size=CONFIG['test_size'],\n",
    "        random_state=42,\n",
    "        use_subset=True,\n",
    "        max_classes=CONFIG['max_classes']\n",
    "    )\n",
    "    \n",
    "    # Test dataset\n",
    "    test_dataset = ImprovedPoseSequenceDataset(\n",
    "        data_dir=DATA_DIR,\n",
    "        max_seq_len=CONFIG['max_seq_len'], \n",
    "        split='test',\n",
    "        test_size=CONFIG['test_size'],\n",
    "        random_state=42,\n",
    "        use_subset=True,\n",
    "        max_classes=CONFIG['max_classes']\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Datasets created successfully!\")\n",
    "    print(f\"üìä Training samples: {len(train_dataset)}\")\n",
    "    print(f\"üìä Test samples: {len(test_dataset)}\")\n",
    "    print(f\"üìä Number of classes: {train_dataset.num_classes}\")\n",
    "    print(f\"üìä Batch size: {CONFIG['batch_size']}\")\n",
    "    \n",
    "    # Save class mapping\n",
    "    class_mapping = {\n",
    "        'word_to_idx': train_dataset.word_to_idx,\n",
    "        'idx_to_word': train_dataset.idx_to_word\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(CHECKPOINT_DIR, 'class_mapping.json'), 'w') as f:\n",
    "        json.dump(class_mapping, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Class mapping saved to {CHECKPOINT_DIR}/class_mapping.json\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot create datasets. Please fix data issues first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891cc429",
   "metadata": {},
   "source": [
    "## üèóÔ∏è TGCN Model Architecture\n",
    "\n",
    "Implement the Temporal Graph Convolutional Network with improved connectivity for 553 nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGCN(nn.Module):\n",
    "    \"\"\"Temporal Graph Convolutional Network for ASL Recognition\"\"\"\n",
    "    \n",
    "    def __init__(self, num_nodes, num_features, num_classes, \n",
    "                 gcn_hidden=256, temporal_kernel=9, dropout=0.3, num_gcn_layers=3):\n",
    "        super(TemporalGCN, self).__init__()\n",
    "        \n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.gcn_hidden = gcn_hidden\n",
    "        self.temporal_kernel = temporal_kernel\n",
    "        \n",
    "        # Create improved graph connectivity\n",
    "        self.edge_index = create_improved_graph_connectivity()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(num_features, gcn_hidden)\n",
    "        \n",
    "        # GCN layers with residual connections\n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        for i in range(num_gcn_layers):\n",
    "            self.gcn_layers.append(GCNConv(gcn_hidden, gcn_hidden))\n",
    "        \n",
    "        # Batch normalization for each GCN layer\n",
    "        self.batch_norms = nn.ModuleList([\n",
    "            nn.BatchNorm1d(gcn_hidden) for _ in range(num_gcn_layers)\n",
    "        ])\n",
    "        \n",
    "        # Temporal convolution layers\n",
    "        self.temporal_conv1 = nn.Conv1d(\n",
    "            gcn_hidden, gcn_hidden, \n",
    "            kernel_size=temporal_kernel,\n",
    "            padding=temporal_kernel//2\n",
    "        )\n",
    "        self.temporal_conv2 = nn.Conv1d(\n",
    "            gcn_hidden, gcn_hidden//2,\n",
    "            kernel_size=temporal_kernel,\n",
    "            padding=temporal_kernel//2\n",
    "        )\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.spatial_dropout = nn.Dropout2d(dropout * 0.5)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(gcn_hidden//2, gcn_hidden//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(gcn_hidden//4, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, num_nodes, num_features]\n",
    "            \n",
    "        Returns:\n",
    "            logits: Class predictions [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, num_nodes, num_features = x.shape\n",
    "        \n",
    "        # Move edge index to same device as input\n",
    "        edge_index = self.edge_index.to(x.device)\n",
    "        \n",
    "        # Process each frame separately\n",
    "        frame_outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Current frame: [batch_size, num_nodes, num_features]\n",
    "            frame = x[:, t, :, :]\n",
    "            \n",
    "            # Project input features\n",
    "            h = self.input_projection(frame)  # [batch_size, num_nodes, gcn_hidden]\n",
    "            \n",
    "            # Apply GCN layers with residual connections\n",
    "            for i, (gcn, bn) in enumerate(zip(self.gcn_layers, self.batch_norms)):\n",
    "                residual = h if i > 0 else None\n",
    "                \n",
    "                # Reshape for GCN: [batch_size * num_nodes, gcn_hidden]\n",
    "                h_flat = h.view(-1, h.size(-1))\n",
    "                \n",
    "                # Expand edge index for batch\n",
    "                batch_edge_index = edge_index.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "                batch_edge_index = batch_edge_index + torch.arange(batch_size, device=x.device).view(-1, 1, 1) * num_nodes\n",
    "                batch_edge_index = batch_edge_index.view(2, -1)\n",
    "                \n",
    "                # Apply GCN\n",
    "                h_flat = gcn(h_flat, batch_edge_index)\n",
    "                h = h_flat.view(batch_size, num_nodes, -1)\n",
    "                \n",
    "                # Batch normalization\n",
    "                h = h.permute(0, 2, 1)  # [batch_size, gcn_hidden, num_nodes]\n",
    "                h = bn(h)\n",
    "                h = h.permute(0, 2, 1)  # [batch_size, num_nodes, gcn_hidden]\n",
    "                \n",
    "                # Activation and residual connection\n",
    "                h = F.relu(h)\n",
    "                if residual is not None:\n",
    "                    h = h + residual\n",
    "                \n",
    "                h = self.dropout(h)\n",
    "            \n",
    "            # Spatial dropout for regularization\n",
    "            h = h.unsqueeze(-1)  # [batch_size, num_nodes, gcn_hidden, 1]\n",
    "            h = self.spatial_dropout(h)\n",
    "            h = h.squeeze(-1)   # [batch_size, num_nodes, gcn_hidden]\n",
    "            \n",
    "            # Global spatial pooling for this frame\n",
    "            frame_features = torch.mean(h, dim=1)  # [batch_size, gcn_hidden]\n",
    "            frame_outputs.append(frame_features)\n",
    "        \n",
    "        # Stack frame features: [batch_size, seq_len, gcn_hidden]\n",
    "        temporal_features = torch.stack(frame_outputs, dim=1)\n",
    "        \n",
    "        # Temporal convolution: [batch_size, gcn_hidden, seq_len]\n",
    "        temporal_features = temporal_features.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply temporal convolutions\n",
    "        temporal_features = F.relu(self.temporal_conv1(temporal_features))\n",
    "        temporal_features = self.dropout(temporal_features)\n",
    "        temporal_features = F.relu(self.temporal_conv2(temporal_features))\n",
    "        \n",
    "        # Global temporal pooling\n",
    "        sequence_features = self.global_pool(temporal_features).squeeze(-1)  # [batch_size, gcn_hidden//2]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(sequence_features)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"‚úÖ TGCN model defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8580f76",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Training Utilities and Metrics\n",
    "\n",
    "Define training utilities, metrics calculation, and progress tracking functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc17ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsTracker:\n",
    "    \"\"\"Track training metrics and progress\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.learning_rates = []\n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc, lr):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.train_accuracies.append(train_acc)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.val_accuracies.append(val_acc)\n",
    "        self.learning_rates.append(lr)\n",
    "        \n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            self.best_epoch = len(self.val_accuracies) - 1\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Plot training metrics\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[0, 0].plot(self.train_losses, label='Train Loss', color='blue')\n",
    "        axes[0, 0].plot(self.val_losses, label='Val Loss', color='red')\n",
    "        axes[0, 0].set_title('Training and Validation Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        axes[0, 1].plot(self.train_accuracies, label='Train Acc', color='blue')\n",
    "        axes[0, 1].plot(self.val_accuracies, label='Val Acc', color='red')\n",
    "        axes[0, 1].axhline(y=self.best_val_acc, color='green', linestyle='--', \n",
    "                          label=f'Best Val Acc: {self.best_val_acc:.3f}')\n",
    "        axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Learning rate plot\n",
    "        axes[1, 0].plot(self.learning_rates, color='orange')\n",
    "        axes[1, 0].set_title('Learning Rate Schedule')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Validation accuracy zoomed\n",
    "        axes[1, 1].plot(self.val_accuracies, color='red', linewidth=2)\n",
    "        axes[1, 1].axhline(y=self.best_val_acc, color='green', linestyle='--')\n",
    "        axes[1, 1].set_title(f'Validation Accuracy (Best: {self.best_val_acc:.3f}% at epoch {self.best_epoch})')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    \"\"\"Calculate accuracy from model outputs and targets\"\"\"\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == targets).sum().item()\n",
    "    total = targets.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, use_augmentation=False):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        # Apply data augmentation if enabled\n",
    "        if use_augmentation and np.random.random() < CONFIG['aug_probability']:\n",
    "            # Convert to numpy for augmentation\n",
    "            data_np = data.cpu().numpy()\n",
    "            \n",
    "            # Apply spatial augmentation\n",
    "            if np.random.random() < 0.5:\n",
    "                data_np = apply_spatial_augmentation(\n",
    "                    data_np, \n",
    "                    scale_range=CONFIG['spatial_aug_strength'],\n",
    "                    translation_range=CONFIG['spatial_aug_strength']\n",
    "                )\n",
    "            \n",
    "            # Apply temporal augmentation  \n",
    "            if np.random.random() < 0.5:\n",
    "                augmented_batch = []\n",
    "                for i in range(data_np.shape[0]):\n",
    "                    aug_seq = apply_temporal_augmentation(\n",
    "                        data_np[i], \n",
    "                        speed_range=CONFIG['temporal_aug_strength']\n",
    "                    )\n",
    "                    # Ensure consistent length\n",
    "                    if aug_seq.shape[0] != CONFIG['max_seq_len']:\n",
    "                        if aug_seq.shape[0] > CONFIG['max_seq_len']:\n",
    "                            indices = np.linspace(0, aug_seq.shape[0]-1, CONFIG['max_seq_len'], dtype=int)\n",
    "                            aug_seq = aug_seq[indices]\n",
    "                        else:\n",
    "                            padding = np.zeros((CONFIG['max_seq_len'] - aug_seq.shape[0], \n",
    "                                              aug_seq.shape[1], aug_seq.shape[2]))\n",
    "                            aug_seq = np.concatenate([aug_seq, padding], axis=0)\n",
    "                    augmented_batch.append(aug_seq)\n",
    "                \n",
    "                data = torch.tensor(np.array(augmented_batch), dtype=torch.float32).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += calculate_accuracy(outputs, targets)\n",
    "        num_batches += 1\n",
    "        \n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f'  Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return total_loss / num_batches, total_accuracy / num_batches\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in val_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += calculate_accuracy(outputs, targets)\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches, total_accuracy / num_batches\n",
    "\n",
    "print(\"‚úÖ Training utilities defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ee451",
   "metadata": {},
   "source": [
    "## üöÄ Model Initialization and Training Setup\n",
    "\n",
    "Initialize the TGCN model and set up the training components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_ready:\n",
    "    # Initialize model\n",
    "    model = TemporalGCN(\n",
    "        num_nodes=CONFIG['num_nodes'],\n",
    "        num_features=CONFIG['num_features'],\n",
    "        num_classes=train_dataset.num_classes,\n",
    "        gcn_hidden=CONFIG['gcn_hidden'],\n",
    "        temporal_kernel=CONFIG['temporal_kernel'],\n",
    "        dropout=CONFIG['dropout'],\n",
    "        num_gcn_layers=CONFIG['num_gcn_layers']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"üèóÔ∏è Model Architecture:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Model size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=CONFIG['min_lr'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Metrics tracker\n",
    "    metrics = MetricsTracker()\n",
    "    \n",
    "    print(f\"‚úÖ Training setup complete!\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    print(f\"  Optimizer: AdamW\")\n",
    "    print(f\"  Scheduler: ReduceLROnPlateau\")\n",
    "    print(f\"  Loss function: CrossEntropyLoss\")\n",
    "    \n",
    "    # Test a forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_batch = next(iter(train_loader))\n",
    "        test_data, test_targets = test_batch[0][:2].to(device), test_batch[1][:2].to(device)\n",
    "        test_output = model(test_data)\n",
    "        print(f\"\\nüß™ Test forward pass:\")\n",
    "        print(f\"  Input shape: {test_data.shape}\")\n",
    "        print(f\"  Output shape: {test_output.shape}\")\n",
    "        print(f\"  Expected output shape: [2, {train_dataset.num_classes}]\")\n",
    "        \n",
    "        if test_output.shape == (2, train_dataset.num_classes):\n",
    "            print(f\"  ‚úÖ Forward pass successful!\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Shape mismatch in forward pass!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize model. Please fix data issues first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5ce932",
   "metadata": {},
   "source": [
    "## üéØ Training Loop\n",
    "\n",
    "Execute the main training loop with validation, early stopping, and progress tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f67338",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_ready:\n",
    "    print(\"üöÄ Starting training...\")\n",
    "    print(f\"Target: 87.60% accuracy on WLASL-100 (literature benchmark)\\n\")\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Training phase\n",
    "        print(\"üèãÔ∏è Training...\")\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, \n",
    "            use_augmentation=CONFIG['use_augmentation']\n",
    "        )\n",
    "        \n",
    "        # Validation phase\n",
    "        print(\"\\nüî¨ Validating...\")\n",
    "        val_loss, val_acc = validate_epoch(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Update metrics\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        metrics.update(train_loss, train_acc, val_loss, val_acc, current_lr)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"\\nüìä Epoch {epoch+1} Results:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  Best Val Acc: {metrics.best_val_acc:.2f}% (Epoch {metrics.best_epoch+1})\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'config': CONFIG,\n",
    "                'class_mapping': class_mapping\n",
    "            }, MODEL_SAVE_PATH)\n",
    "            \n",
    "            print(f\"  üíæ New best model saved! Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            # Check if we've reached the target accuracy\n",
    "            if val_acc >= 87.60:\n",
    "                print(f\"\\nüéâ TARGET ACHIEVED! Validation accuracy: {val_acc:.2f}% >= 87.60%\")\n",
    "                print(f\"Training completed successfully at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  ‚è≥ Patience: {patience_counter}/{CONFIG['patience']}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(f\"\\n‚õî Early stopping triggered at epoch {epoch+1}\")\n",
    "                print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        # Stop if learning rate becomes too small\n",
    "        if current_lr < CONFIG['min_lr']:\n",
    "            print(f\"\\n‚õî Learning rate too small: {current_lr:.2e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nüèÅ Training completed!\")\n",
    "    print(f\"Best validation accuracy: {metrics.best_val_acc:.2f}%\")\n",
    "    print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n",
    "    \n",
    "    # Plot training metrics\n",
    "    print(\"\\nüìà Training Metrics:\")\n",
    "    metrics.plot_metrics()\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot start training. Please fix data issues first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769cb4f",
   "metadata": {},
   "source": [
    "## üìä Model Evaluation and Analysis\n",
    "\n",
    "Evaluate the trained model and analyze its performance in detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, class_mapping, device):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_outputs.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    \n",
    "    # Classification report\n",
    "    idx_to_word = class_mapping['idx_to_word']\n",
    "    class_names = [idx_to_word[str(i)] for i in range(len(idx_to_word))]\n",
    "    \n",
    "    print(f\"\\nüìä Final Evaluation Results:\")\n",
    "    print(f\"Overall Accuracy: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    report = classification_report(\n",
    "        all_targets, all_predictions, \n",
    "        target_names=class_names,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìã Classification Report:\")\n",
    "    print(classification_report(all_targets, all_predictions, target_names=class_names))\n",
    "    \n",
    "    # Confusion matrix visualization\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names[:20],  # Show first 20 classes\n",
    "                yticklabels=class_names[:20])\n",
    "    plt.title('Confusion Matrix (First 20 Classes)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Per-class accuracy analysis\n",
    "    class_accuracies = []\n",
    "    for i in range(len(class_names)):\n",
    "        class_mask = np.array(all_targets) == i\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = accuracy_score(\n",
    "                np.array(all_targets)[class_mask],\n",
    "                np.array(all_predictions)[class_mask]\n",
    "            )\n",
    "            class_accuracies.append((class_names[i], class_acc, np.sum(class_mask)))\n",
    "    \n",
    "    # Sort by accuracy\n",
    "    class_accuracies.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nüéØ Top 10 Best Performing Classes:\")\n",
    "    for name, acc, count in class_accuracies[:10]:\n",
    "        print(f\"  {name}: {acc*100:.1f}% ({count} samples)\")\n",
    "    \n",
    "    print(f\"\\nüéØ Bottom 10 Performing Classes:\")\n",
    "    for name, acc, count in class_accuracies[-10:]:\n",
    "        print(f\"  {name}: {acc*100:.1f}% ({count} samples)\")\n",
    "    \n",
    "    return accuracy, report, cm\n",
    "\n",
    "if data_ready and os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(\"üîÑ Loading best model for evaluation...\")\n",
    "    \n",
    "    # Load the best model\n",
    "    checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded from epoch {checkpoint['epoch']+1}\")\n",
    "    print(f\"Best validation accuracy: {checkpoint['best_val_acc']:.2f}%\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    accuracy, report, cm = evaluate_model(model, test_loader, class_mapping, device)\n",
    "    \n",
    "    # Compare with literature\n",
    "    print(f\"\\nüèÜ Performance Comparison:\")\n",
    "    print(f\"Our Model: {accuracy*100:.2f}%\")\n",
    "    print(f\"Literature Benchmark (WLASL-100): 87.60%\")\n",
    "    \n",
    "    if accuracy*100 >= 87.60:\n",
    "        print(f\"üéâ SUCCESS! We've achieved the literature benchmark!\")\n",
    "    elif accuracy*100 >= 80.0:\n",
    "        print(f\"‚úÖ GOOD! Strong performance, close to benchmark\")\n",
    "    elif accuracy*100 >= 70.0:\n",
    "        print(f\"‚ö†Ô∏è MODERATE: Decent performance, room for improvement\")\n",
    "    else:\n",
    "        print(f\"‚ùå POOR: Significant improvement needed\")\n",
    "    \n",
    "    # Save evaluation results\n",
    "    eval_results = {\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': report,\n",
    "        'model_path': MODEL_SAVE_PATH,\n",
    "        'config': CONFIG\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(CHECKPOINT_DIR, 'evaluation_results.json'), 'w') as f:\n",
    "        json.dump(eval_results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nüíæ Evaluation results saved to {CHECKPOINT_DIR}/evaluation_results.json\")\n",
    "\n",
    "else:\n",
    "    if not data_ready:\n",
    "        print(\"‚ùå Cannot evaluate: Data not ready\")\n",
    "    else:\n",
    "        print(f\"‚ùå Cannot evaluate: Model not found at {MODEL_SAVE_PATH}\")\n",
    "        print(\"Please run the training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b688d18",
   "metadata": {},
   "source": [
    "## üîÆ Inference and Model Usage\n",
    "\n",
    "Functions for using the trained model to make predictions on new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7716d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path, device):\n",
    "    \"\"\"Load a trained model for inference\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    config = checkpoint['config']\n",
    "    class_mapping = checkpoint['class_mapping']\n",
    "    \n",
    "    # Initialize model with saved config\n",
    "    model = TemporalGCN(\n",
    "        num_nodes=config['num_nodes'],\n",
    "        num_features=config['num_features'],\n",
    "        num_classes=len(class_mapping['idx_to_word']),\n",
    "        gcn_hidden=config['gcn_hidden'],\n",
    "        temporal_kernel=config['temporal_kernel'],\n",
    "        dropout=config['dropout'],\n",
    "        num_gcn_layers=config['num_gcn_layers']\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, class_mapping, config\n",
    "\n",
    "def predict_sequence(model, keypoints, class_mapping, config, device):\n",
    "    \"\"\"\n",
    "    Predict ASL sign from keypoint sequence\n",
    "    \n",
    "    Args:\n",
    "        model: Trained TGCN model\n",
    "        keypoints: numpy array [seq_len, num_nodes, 3]\n",
    "        class_mapping: Dictionary with class mappings\n",
    "        config: Model configuration\n",
    "        device: PyTorch device\n",
    "    \n",
    "    Returns:\n",
    "        predicted_class: string\n",
    "        confidence: float\n",
    "        all_probabilities: dict\n",
    "    \"\"\"\n",
    "    # Normalize keypoints\n",
    "    normalizer = ImprovedPoseNormalizer()\n",
    "    normalized_keypoints = normalizer.normalize_pose_sequence(keypoints)\n",
    "    \n",
    "    # Handle sequence length\n",
    "    seq_len = normalized_keypoints.shape[0]\n",
    "    if seq_len > config['max_seq_len']:\n",
    "        # Resample to max length\n",
    "        indices = np.linspace(0, seq_len - 1, config['max_seq_len'], dtype=int)\n",
    "        normalized_keypoints = normalized_keypoints[indices]\n",
    "    elif seq_len < config['max_seq_len']:\n",
    "        # Pad sequence\n",
    "        padding = np.zeros((config['max_seq_len'] - seq_len, \n",
    "                          normalized_keypoints.shape[1], \n",
    "                          normalized_keypoints.shape[2]))\n",
    "        normalized_keypoints = np.concatenate([normalized_keypoints, padding], axis=0)\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    input_tensor = torch.tensor(normalized_keypoints, dtype=torch.float32)\n",
    "    input_tensor = input_tensor.unsqueeze(0).to(device)  # [1, seq_len, num_nodes, 3]\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "    \n",
    "    # Convert to readable format\n",
    "    predicted_class = class_mapping['idx_to_word'][str(predicted_idx.item())]\n",
    "    confidence_score = confidence.item()\n",
    "    \n",
    "    # Get all class probabilities\n",
    "    all_probs = {}\n",
    "    prob_array = probabilities[0].cpu().numpy()\n",
    "    for idx, prob in enumerate(prob_array):\n",
    "        if str(idx) in class_mapping['idx_to_word']:\n",
    "            class_name = class_mapping['idx_to_word'][str(idx)]\n",
    "            all_probs[class_name] = float(prob)\n",
    "    \n",
    "    return predicted_class, confidence_score, all_probs\n",
    "\n",
    "def inference_demo():\n",
    "    \"\"\"Demonstrate inference on a sample from the test set\"\"\"\n",
    "    if not data_ready or not os.path.exists(MODEL_SAVE_PATH):\n",
    "        print(\"‚ùå Cannot run demo: Model or data not available\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîÆ Running inference demo...\")\n",
    "    \n",
    "    # Load trained model\n",
    "    model, class_mapping, config = load_trained_model(MODEL_SAVE_PATH, device)\n",
    "    \n",
    "    # Get a random sample from test set\n",
    "    test_sample_idx = np.random.randint(0, len(test_dataset))\n",
    "    sample_data, true_label = test_dataset[test_sample_idx]\n",
    "    \n",
    "    # Convert tensor back to numpy for prediction function\n",
    "    keypoints_np = sample_data.cpu().numpy()  # [seq_len, num_nodes, 3]\n",
    "    true_class = class_mapping['idx_to_word'][str(true_label.item())]\n",
    "    \n",
    "    # Make prediction\n",
    "    predicted_class, confidence, all_probs = predict_sequence(\n",
    "        model, keypoints_np, class_mapping, config, device\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéØ Inference Results:\")\n",
    "    print(f\"True class: {true_class}\")\n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "    print(f\"Confidence: {confidence:.3f}\")\n",
    "    print(f\"Correct: {'‚úÖ' if predicted_class == true_class else '‚ùå'}\")\n",
    "    \n",
    "    # Show top 5 predictions\n",
    "    sorted_probs = sorted(all_probs.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"\\nüìä Top 5 Predictions:\")\n",
    "    for i, (class_name, prob) in enumerate(sorted_probs[:5]):\n",
    "        marker = \"üëë\" if class_name == true_class else \"  \"\n",
    "        print(f\"  {marker} {i+1}. {class_name}: {prob:.3f}\")\n",
    "\n",
    "# Run inference demo\n",
    "if data_ready:\n",
    "    inference_demo()\n",
    "\n",
    "print(\"\\n‚úÖ Inference functions defined successfully!\")\n",
    "print(\"üí° Use predict_sequence() function to make predictions on new keypoint data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa84e1b",
   "metadata": {},
   "source": [
    "## üìã Summary and Next Steps\n",
    "\n",
    "### üéâ What We've Accomplished\n",
    "\n",
    "1. **Enhanced Architecture**: 553-node TGCN with pose, hands, and face landmarks\n",
    "2. **Advanced Preprocessing**: Spatial anchoring, temporal smoothing, interpolation\n",
    "3. **Improved Connectivity**: Anatomical and functional graph relationships\n",
    "4. **Data Augmentation**: Spatial and temporal transformations\n",
    "5. **Robust Training**: Early stopping, learning rate scheduling, gradient clipping\n",
    "6. **Comprehensive Evaluation**: Metrics, visualizations, and analysis\n",
    "\n",
    "### üéØ Performance Target\n",
    "\n",
    "- **Goal**: 87.60% accuracy on WLASL-100 (literature benchmark)\n",
    "- **Architecture**: 553 nodes (33 pose + 42 hands + 478 face)\n",
    "- **Dataset**: WLASL-100 subset for quality training\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. **Data Extraction**: Run `pose_estimation_mediapipe.py` to extract 553-node keypoints\n",
    "2. **Training**: Execute this notebook to train the TGCN model\n",
    "3. **Evaluation**: Analyze performance and compare with literature\n",
    "4. **Optimization**: Tune hyperparameters if needed\n",
    "5. **Production**: Use inference functions for real-time ASL recognition\n",
    "\n",
    "### üìÅ File Structure\n",
    "\n",
    "```\n",
    "pose_estimation/src/\n",
    "‚îú‚îÄ‚îÄ tgcn_pipeline.ipynb     # This comprehensive notebook\n",
    "‚îú‚îÄ‚îÄ normalization.py        # Advanced preprocessing utilities\n",
    "‚îî‚îÄ‚îÄ preprocessing/\n",
    "    ‚îî‚îÄ‚îÄ pose_estimation_mediapipe.py  # 553-node keypoint extraction\n",
    "```\n",
    "\n",
    "### üîß Key Configuration\n",
    "\n",
    "- **Nodes**: 553 (pose + hands + face)\n",
    "- **Sequence Length**: 50 frames\n",
    "- **Batch Size**: 16 (optimized for 553 nodes)\n",
    "- **Classes**: 100 (WLASL-100 subset)\n",
    "- **Augmentation**: Spatial + temporal\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to achieve state-of-the-art ASL recognition performance! üöÄ**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61926f86",
   "metadata": {},
   "source": [
    "## üß™ Quick Test: Verify 553-Node Keypoint Format\n",
    "\n",
    "Before training, let's verify that our keypoint extraction produced the correct 553-node format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ac7c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing 553-node keypoint extraction format...\n",
      "üìÅ Testing 3 sample files...\n",
      "\n",
      "1. about/00414_keypoints.npz\n",
      "   ‚úÖ Correct format: 104 frames √ó 553 nodes √ó 3 coords\n",
      "   üéØ Detection: Pose ‚úÖ | Hands ‚ùå | Face ‚úÖ\n",
      "\n",
      "2. about/00415_keypoints.npz\n",
      "   ‚úÖ Correct format: 37 frames √ó 553 nodes √ó 3 coords\n",
      "   üéØ Detection: Pose ‚úÖ | Hands ‚ùå | Face ‚úÖ\n",
      "\n",
      "3. about/00416_keypoints.npz\n",
      "   ‚úÖ Correct format: 115 frames √ó 553 nodes √ó 3 coords\n",
      "   üéØ Detection: Pose ‚úÖ | Hands ‚ùå | Face ‚úÖ\n",
      "\n",
      "üéâ SUCCESS: All tested files have correct 553-node format!\n",
      "üìä Total keypoint files available: 203\n",
      "\n",
      "‚úÖ Ready to proceed with training!\n",
      "   ‚úÖ Correct format: 37 frames √ó 553 nodes √ó 3 coords\n",
      "   üéØ Detection: Pose ‚úÖ | Hands ‚ùå | Face ‚úÖ\n",
      "\n",
      "3. about/00416_keypoints.npz\n",
      "   ‚úÖ Correct format: 115 frames √ó 553 nodes √ó 3 coords\n",
      "   üéØ Detection: Pose ‚úÖ | Hands ‚ùå | Face ‚úÖ\n",
      "\n",
      "üéâ SUCCESS: All tested files have correct 553-node format!\n",
      "üìä Total keypoint files available: 203\n",
      "\n",
      "‚úÖ Ready to proceed with training!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def test_keypoint_format_quick():\n",
    "    \"\"\"Quick test to verify 553-node keypoint format\"\"\"\n",
    "    print(\"üîç Testing 553-node keypoint extraction format...\")\n",
    "    \n",
    "    keypoints_dir = Path(DATA_DIR)\n",
    "    keypoint_files = list(keypoints_dir.glob(\"*/*_keypoints.npz\"))\n",
    "    \n",
    "    if not keypoint_files:\n",
    "        print(\"‚ùå No keypoint files found. Run pose_estimation_mediapipe.py first!\")\n",
    "        return False\n",
    "    \n",
    "    # Test first few files\n",
    "    test_files = keypoint_files[:3]\n",
    "    print(f\"üìÅ Testing {len(test_files)} sample files...\")\n",
    "    \n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"\\n{i+1}. {file_path.parent.name}/{file_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            data = np.load(str(file_path))\n",
    "            \n",
    "            if 'nodes' in data:\n",
    "                nodes = data['nodes']\n",
    "                frames, num_nodes, coords = nodes.shape\n",
    "                \n",
    "                if num_nodes == 553 and coords == 3:\n",
    "                    print(f\"   ‚úÖ Correct format: {frames} frames √ó 553 nodes √ó 3 coords\")\n",
    "                    \n",
    "                    # Quick detection check\n",
    "                    pose_detected = np.any(nodes[0, 0:33, :] != 0)\n",
    "                    hands_detected = np.any(nodes[0, 33:75, :] != 0)\n",
    "                    face_detected = np.any(nodes[0, 75:553, :] != 0)\n",
    "                    \n",
    "                    print(f\"   üéØ Detection: Pose {'‚úÖ' if pose_detected else '‚ùå'} | \"\n",
    "                          f\"Hands {'‚úÖ' if hands_detected else '‚ùå'} | \"\n",
    "                          f\"Face {'‚úÖ' if face_detected else '‚ùå'}\")\n",
    "                    \n",
    "                elif num_nodes == 75:\n",
    "                    print(f\"   ‚ö†Ô∏è  Old format detected: {frames} frames √ó 75 nodes √ó 3 coords\")\n",
    "                    print(f\"   üí° Re-run pose_estimation_mediapipe.py for 553-node extraction\")\n",
    "                    return False\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Unexpected format: {frames} frames √ó {num_nodes} nodes √ó {coords} coords\")\n",
    "                    return False\n",
    "            else:\n",
    "                print(f\"   ‚ùå No 'nodes' key found\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    print(f\"\\nüéâ SUCCESS: All tested files have correct 553-node format!\")\n",
    "    print(f\"üìä Total keypoint files available: {len(keypoint_files)}\")\n",
    "    return True\n",
    "\n",
    "# Run the test\n",
    "format_test_passed = test_keypoint_format_quick()\n",
    "\n",
    "if format_test_passed:\n",
    "    print(\"\\n‚úÖ Ready to proceed with training!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Please fix keypoint format before training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d89e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "146d6788",
   "metadata": {},
   "source": [
    "## üö® CRITICAL: Debug Hand Detection First\n",
    "\n",
    "Before running the full keypoint extraction, let's debug hand detection to ensure ASL hands are properly detected. This is CRITICAL for ASL recognition success.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8adf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Run hand detection debugging first!\n",
    "# This will test MediaPipe hand detection on sample ASL videos\n",
    "\n",
    "print(\"üö® CRITICAL: Testing hand detection before full extraction\")\n",
    "print(\"This step is essential for ASL recognition success!\")\n",
    "print(\"\")\n",
    "\n",
    "# Check if debug script exists\n",
    "import os\n",
    "debug_script = r'f:\\Uni_Stuff\\6th_Sem\\DL\\Proj\\video-asl-recognition\\pose_estimation\\src\\preprocessing\\debug_hand_detection.py'\n",
    "\n",
    "if os.path.exists(debug_script):\n",
    "    print(\"‚úÖ Debug script found. Run this in terminal:\")\n",
    "    print(\"\")\n",
    "    print(\"cd \\\"f:\\\\Uni_Stuff\\\\6th_Sem\\\\DL\\\\Proj\\\\video-asl-recognition\\\\pose_estimation\\\\src\\\\preprocessing\\\"\")\n",
    "    print(\"python debug_hand_detection.py\")\n",
    "    print(\"\")\n",
    "    print(\"This will:\")\n",
    "    print(\"1. üîç Test hand detection on sample ASL videos\")\n",
    "    print(\"2. üìä Show detection rates for hands, pose, and face\")\n",
    "    print(\"3. üíæ Create debug visualizations\")\n",
    "    print(\"4. üí° Provide recommendations for optimization\")\n",
    "    print(\"\")\n",
    "    print(\"‚ùó IMPORTANT: Only proceed with full extraction if hand detection >30%\")\n",
    "    print(\"‚ùó If hand detection is poor, we'll need to fix MediaPipe settings first\")\n",
    "else:\n",
    "    print(\"‚ùå Debug script not found. Creating it now...\")\n",
    "    # The script was already created above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aslpose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
