{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db43fcdf",
   "metadata": {},
   "source": [
    "# ASL Recognition: End-to-End Workflow\n",
    "\n",
    "This notebook consolidates the entire workflow for preprocessing, model training, and evaluation for American Sign Language (ASL) recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a040ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm  # Use notebook-friendly version of tqdm\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b7725",
   "metadata": {},
   "source": [
    "# Preprocessing: Load and Normalize Keypoints\n",
    "\n",
    "We will load the preprocessed keypoints from the JSON files and normalize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4673b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading keypoints from f:\\Uni_Stuff\\6th_Sem\\DL\\Proj\\video-asl-recognition\\pose_estimation\\data\\keypoints\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b503d75873e4a07a7e127820e1291b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading keypoint files:   0%|          | 0/3202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3202 files successfully, skipped 0 files\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(r'f:\\Uni_Stuff\\6th_Sem\\DL\\Proj\\video-asl-recognition\\pose_estimation\\data\\keypoints')\n",
    "\n",
    "# Load and normalize keypoints\n",
    "# Now also return sequence lengths for masking\n",
    "\n",
    "def load_keypoints(data_dir):\n",
    "    data = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    skipped_files = 0\n",
    "    processed_files = 0\n",
    "    print(f\"Loading keypoints from {data_dir}\")\n",
    "    # Gather all json files from all label directories\n",
    "    all_json_files = []\n",
    "    for label_dir in data_dir.iterdir():\n",
    "        if label_dir.is_dir():\n",
    "            all_json_files.extend(list(label_dir.glob('*.json')))\n",
    "    for json_file in tqdm(all_json_files, desc=\"Loading keypoint files\"):\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                if 'keypoints' not in content or 'label' not in content:\n",
    "                    skipped_files += 1\n",
    "                    continue\n",
    "                keypoints = content['keypoints']\n",
    "                label = content['label']\n",
    "                processed_frames = []\n",
    "                for frame in keypoints:\n",
    "                    left_hand_features = np.zeros(63, dtype=np.float32)\n",
    "                    right_hand_features = np.zeros(63, dtype=np.float32)\n",
    "                    pose_features = np.zeros(99, dtype=np.float32)\n",
    "                    if 'hands' in frame and frame['hands']:\n",
    "                        for i, hand in enumerate(frame['hands']):\n",
    "                            if i < 2:\n",
    "                                hand_features = []\n",
    "                                for point in hand:\n",
    "                                    if isinstance(point, list) and len(point) == 3:\n",
    "                                        hand_features.extend(point)\n",
    "                                if i == 0 and len(hand_features) <= 63:\n",
    "                                    left_hand_features[:len(hand_features)] = hand_features\n",
    "                                elif i == 1 and len(hand_features) <= 63:\n",
    "                                    right_hand_features[:len(hand_features)] = hand_features\n",
    "                    if 'pose' in frame and frame['pose']:\n",
    "                        pose_data = []\n",
    "                        for point in frame['pose']:\n",
    "                            if isinstance(point, list) and len(point) == 3:\n",
    "                                pose_data.extend(point)\n",
    "                        if len(pose_data) <= 99:\n",
    "                            pose_features[:len(pose_data)] = pose_data\n",
    "                    frame_features = np.concatenate([left_hand_features, right_hand_features, pose_features])\n",
    "                    processed_frames.append(frame_features)\n",
    "                if processed_frames:\n",
    "                    processed_data = np.array(processed_frames, dtype=np.float32)\n",
    "                    if processed_data.shape[0] > 0 and processed_data.shape[1] > 0:\n",
    "                        data.append(processed_data)\n",
    "                        labels.append(label)\n",
    "                        lengths.append(processed_data.shape[0])\n",
    "                        processed_files += 1\n",
    "                    else:\n",
    "                        skipped_files += 1\n",
    "                else:\n",
    "                    skipped_files += 1\n",
    "        except Exception as e:\n",
    "            skipped_files += 1\n",
    "    print(f\"Processed {processed_files} files successfully, skipped {skipped_files} files\")\n",
    "    if not data:\n",
    "        print(\"Warning: No valid data was loaded!\")\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "    max_seq_len = max(sample.shape[0] for sample in data)\n",
    "    feature_dim = data[0].shape[1]\n",
    "    padded_data = []\n",
    "    for sample in data:\n",
    "        if sample.shape[0] < max_seq_len:\n",
    "            padding = np.zeros((max_seq_len - sample.shape[0], feature_dim), dtype=np.float32)\n",
    "            padded_sample = np.vstack((sample, padding))\n",
    "        else:\n",
    "            padded_sample = sample[:max_seq_len]\n",
    "        padded_data.append(padded_sample)\n",
    "    return np.array(padded_data), np.array(labels), np.array(lengths)\n",
    "\n",
    "data, labels, lengths = load_keypoints(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50aa2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 labels:\n",
      "['about' 'about' 'about' 'about' 'about']\n",
      "First 5 lengths:\n",
      "[60 60 60 60 60]\n",
      "Raw data statistics:\n",
      "Number of samples: 3202\n",
      "Shape of sample 0: (60, 225)\n",
      "Sample 0 - min: 0.0 max: 1.0 mean: 0.5605123\n",
      "Raw data stddev: 0.13998298\n",
      "Label distribution:\n",
      "Label: about, Count: 8\n",
      "Label: accident, Count: 13\n",
      "Label: africa, Count: 13\n",
      "Label: again, Count: 10\n",
      "Label: all, Count: 13\n",
      "Label: always, Count: 9\n",
      "Label: animal, Count: 10\n",
      "Label: apple, Count: 13\n",
      "Label: approve, Count: 11\n",
      "Label: argue, Count: 10\n",
      "Label: arrive, Count: 10\n",
      "Label: baby, Count: 10\n",
      "Label: back, Count: 7\n",
      "Label: backpack, Count: 11\n",
      "Label: bad, Count: 11\n",
      "Label: bake, Count: 8\n",
      "Label: balance, Count: 11\n",
      "Label: ball, Count: 11\n",
      "Label: banana, Count: 10\n",
      "Label: bar, Count: 10\n",
      "Label: basketball, Count: 12\n",
      "Label: bath, Count: 10\n",
      "Label: bathroom, Count: 10\n",
      "Label: beard, Count: 10\n",
      "Label: because, Count: 7\n",
      "Label: bed, Count: 13\n",
      "Label: before, Count: 17\n",
      "Label: behind, Count: 9\n",
      "Label: bird, Count: 12\n",
      "Label: birthday, Count: 9\n",
      "Label: black, Count: 13\n",
      "Label: blanket, Count: 8\n",
      "Label: blue, Count: 12\n",
      "Label: book, Count: 10\n",
      "Label: bowling, Count: 13\n",
      "Label: boy, Count: 10\n",
      "Label: bring, Count: 10\n",
      "Label: brother, Count: 11\n",
      "Label: brown, Count: 11\n",
      "Label: business, Count: 11\n",
      "Label: but, Count: 10\n",
      "Label: buy, Count: 11\n",
      "Label: call, Count: 11\n",
      "Label: can, Count: 12\n",
      "Label: candy, Count: 14\n",
      "Label: careful, Count: 8\n",
      "Label: cat, Count: 11\n",
      "Label: catch, Count: 9\n",
      "Label: center, Count: 10\n",
      "Label: cereal, Count: 9\n",
      "Label: chair, Count: 9\n",
      "Label: champion, Count: 8\n",
      "Label: change, Count: 12\n",
      "Label: chat, Count: 10\n",
      "Label: cheat, Count: 12\n",
      "Label: check, Count: 13\n",
      "Label: cheese, Count: 9\n",
      "Label: children, Count: 8\n",
      "Label: christmas, Count: 9\n",
      "Label: city, Count: 13\n",
      "Label: class, Count: 10\n",
      "Label: clock, Count: 7\n",
      "Label: close, Count: 8\n",
      "Label: clothes, Count: 9\n",
      "Label: coffee, Count: 9\n",
      "Label: cold, Count: 12\n",
      "Label: college, Count: 11\n",
      "Label: color, Count: 14\n",
      "Label: computer, Count: 20\n",
      "Label: convince, Count: 11\n",
      "Label: cook, Count: 12\n",
      "Label: cool, Count: 13\n",
      "Label: copy, Count: 9\n",
      "Label: corn, Count: 12\n",
      "Label: cough, Count: 9\n",
      "Label: country, Count: 10\n",
      "Label: cousin, Count: 15\n",
      "Label: cow, Count: 13\n",
      "Label: crash, Count: 11\n",
      "Label: crazy, Count: 9\n",
      "Label: cry, Count: 10\n",
      "Label: cute, Count: 8\n",
      "Label: dance, Count: 11\n",
      "Label: dark, Count: 13\n",
      "Label: daughter, Count: 11\n",
      "Label: day, Count: 10\n",
      "Label: deaf, Count: 13\n",
      "Label: decide, Count: 11\n",
      "Label: delay, Count: 9\n",
      "Label: delicious, Count: 10\n",
      "Label: different, Count: 11\n",
      "Label: disappear, Count: 9\n",
      "Label: discuss, Count: 11\n",
      "Label: divorce, Count: 7\n",
      "Label: doctor, Count: 10\n",
      "Label: dog, Count: 14\n",
      "Label: door, Count: 10\n",
      "Label: draw, Count: 10\n",
      "Label: dress, Count: 9\n",
      "Label: drink, Count: 21\n",
      "Label: drive, Count: 8\n",
      "Label: drop, Count: 10\n",
      "Label: east, Count: 8\n",
      "Label: easy, Count: 8\n",
      "Label: eat, Count: 9\n",
      "Label: egg, Count: 8\n",
      "Label: enjoy, Count: 10\n",
      "Label: environment, Count: 9\n",
      "Label: example, Count: 8\n",
      "Label: family, Count: 14\n",
      "Label: far, Count: 12\n",
      "Label: fat, Count: 11\n",
      "Label: father, Count: 10\n",
      "Label: fault, Count: 7\n",
      "Label: feel, Count: 9\n",
      "Label: fine, Count: 14\n",
      "Label: finish, Count: 12\n",
      "Label: first, Count: 9\n",
      "Label: fish, Count: 14\n",
      "Label: flower, Count: 9\n",
      "Label: football, Count: 9\n",
      "Label: forget, Count: 14\n",
      "Label: friend, Count: 8\n",
      "Label: friendly, Count: 8\n",
      "Label: full, Count: 12\n",
      "Label: future, Count: 9\n",
      "Label: game, Count: 9\n",
      "Label: girl, Count: 10\n",
      "Label: give, Count: 13\n",
      "Label: glasses, Count: 10\n",
      "Label: go, Count: 17\n",
      "Label: good, Count: 10\n",
      "Label: government, Count: 11\n",
      "Label: graduate, Count: 12\n",
      "Label: green, Count: 10\n",
      "Label: hair, Count: 14\n",
      "Label: halloween, Count: 9\n",
      "Label: happy, Count: 10\n",
      "Label: hard, Count: 7\n",
      "Label: hat, Count: 13\n",
      "Label: have, Count: 8\n",
      "Label: headache, Count: 13\n",
      "Label: hear, Count: 10\n",
      "Label: hearing, Count: 14\n",
      "Label: heart, Count: 9\n",
      "Label: help, Count: 14\n",
      "Label: here, Count: 9\n",
      "Label: home, Count: 9\n",
      "Label: hope, Count: 11\n",
      "Label: hot, Count: 15\n",
      "Label: hour, Count: 9\n",
      "Label: house, Count: 9\n",
      "Label: how, Count: 9\n",
      "Label: humble, Count: 10\n",
      "Label: hurry, Count: 7\n",
      "Label: husband, Count: 9\n",
      "Label: improve, Count: 9\n",
      "Label: inform, Count: 9\n",
      "Label: interest, Count: 12\n",
      "Label: internet, Count: 10\n",
      "Label: jacket, Count: 10\n",
      "Label: join, Count: 10\n",
      "Label: jump, Count: 8\n",
      "Label: kill, Count: 9\n",
      "Label: kiss, Count: 11\n",
      "Label: knife, Count: 6\n",
      "Label: know, Count: 11\n",
      "Label: language, Count: 13\n",
      "Label: last, Count: 12\n",
      "Label: late, Count: 10\n",
      "Label: later, Count: 13\n",
      "Label: laugh, Count: 12\n",
      "Label: law, Count: 10\n",
      "Label: learn, Count: 11\n",
      "Label: leave, Count: 12\n",
      "Label: letter, Count: 12\n",
      "Label: light, Count: 11\n",
      "Label: like, Count: 17\n",
      "Label: list, Count: 12\n",
      "Label: live, Count: 6\n",
      "Label: lose, Count: 12\n",
      "Label: make, Count: 9\n",
      "Label: man, Count: 13\n",
      "Label: many, Count: 13\n",
      "Label: match, Count: 10\n",
      "Label: mean, Count: 8\n",
      "Label: meat, Count: 9\n",
      "Label: medicine, Count: 9\n",
      "Label: meet, Count: 13\n",
      "Label: milk, Count: 9\n",
      "Label: money, Count: 8\n",
      "Label: more, Count: 9\n",
      "Label: most, Count: 10\n",
      "Label: mother, Count: 16\n",
      "Label: movie, Count: 7\n",
      "Label: music, Count: 10\n",
      "Label: name, Count: 11\n",
      "Label: need, Count: 11\n",
      "Label: new, Count: 10\n",
      "Label: no, Count: 15\n",
      "Label: none, Count: 8\n",
      "Label: now, Count: 14\n",
      "Label: office, Count: 10\n",
      "Label: old, Count: 8\n",
      "Label: orange, Count: 14\n",
      "Label: order, Count: 8\n",
      "Label: paint, Count: 11\n",
      "Label: pants, Count: 10\n",
      "Label: paper, Count: 12\n",
      "Label: party, Count: 9\n",
      "Label: past, Count: 10\n",
      "Label: pencil, Count: 7\n",
      "Label: person, Count: 10\n",
      "Label: pink, Count: 12\n",
      "Label: pizza, Count: 15\n",
      "Label: plan, Count: 9\n",
      "Label: play, Count: 13\n",
      "Label: please, Count: 8\n",
      "Label: police, Count: 10\n",
      "Label: practice, Count: 8\n",
      "Label: president, Count: 11\n",
      "Label: problem, Count: 9\n",
      "Label: pull, Count: 12\n",
      "Label: purple, Count: 11\n",
      "Label: rabbit, Count: 11\n",
      "Label: read, Count: 9\n",
      "Label: red, Count: 9\n",
      "Label: remember, Count: 10\n",
      "Label: restaurant, Count: 8\n",
      "Label: ride, Count: 8\n",
      "Label: right, Count: 12\n",
      "Label: room, Count: 10\n",
      "Label: run, Count: 10\n",
      "Label: russia, Count: 11\n",
      "Label: salt, Count: 11\n",
      "Label: same, Count: 12\n",
      "Label: sandwich, Count: 11\n",
      "Label: school, Count: 11\n",
      "Label: secretary, Count: 13\n",
      "Label: share, Count: 10\n",
      "Label: shirt, Count: 14\n",
      "Label: short, Count: 13\n",
      "Label: show, Count: 10\n",
      "Label: sick, Count: 11\n",
      "Label: sign, Count: 9\n",
      "Label: since, Count: 9\n",
      "Label: small, Count: 10\n",
      "Label: snow, Count: 8\n",
      "Label: some, Count: 9\n",
      "Label: son, Count: 12\n",
      "Label: soon, Count: 10\n",
      "Label: south, Count: 10\n",
      "Label: stay, Count: 9\n",
      "Label: student, Count: 11\n",
      "Label: study, Count: 15\n",
      "Label: sunday, Count: 11\n",
      "Label: table, Count: 8\n",
      "Label: take, Count: 11\n",
      "Label: tall, Count: 14\n",
      "Label: tea, Count: 9\n",
      "Label: teach, Count: 10\n",
      "Label: teacher, Count: 12\n",
      "Label: tell, Count: 12\n",
      "Label: test, Count: 8\n",
      "Label: thanksgiving, Count: 15\n",
      "Label: theory, Count: 9\n",
      "Label: thin, Count: 16\n",
      "Label: thursday, Count: 13\n",
      "Label: time, Count: 10\n",
      "Label: tired, Count: 10\n",
      "Label: tomato, Count: 8\n",
      "Label: trade, Count: 11\n",
      "Label: train, Count: 9\n",
      "Label: travel, Count: 10\n",
      "Label: ugly, Count: 11\n",
      "Label: visit, Count: 12\n",
      "Label: wait, Count: 13\n",
      "Label: walk, Count: 11\n",
      "Label: want, Count: 11\n",
      "Label: war, Count: 9\n",
      "Label: water, Count: 12\n",
      "Label: week, Count: 11\n",
      "Label: what, Count: 10\n",
      "Label: where, Count: 9\n",
      "Label: white, Count: 13\n",
      "Label: who, Count: 15\n",
      "Label: why, Count: 8\n",
      "Label: wife, Count: 11\n",
      "Label: window, Count: 8\n",
      "Label: with, Count: 10\n",
      "Label: woman, Count: 13\n",
      "Label: work, Count: 12\n",
      "Label: write, Count: 12\n",
      "Label: wrong, Count: 12\n",
      "Label: year, Count: 13\n",
      "Label: yellow, Count: 11\n",
      "Label: yes, Count: 15\n",
      "Label: yesterday, Count: 12\n",
      "Label: you, Count: 12\n",
      "Label: your, Count: 11\n",
      "Sequence lengths statistics:\n",
      "Min length: 60 Max length: 60 Mean length: 60.0\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 labels:\")\n",
    "print(labels[:5])\n",
    "print(\"First 5 lengths:\")\n",
    "print(lengths[:5])\n",
    "\n",
    "\n",
    "# After data, labels, lengths = load_keypoints(DATA_DIR)\n",
    "print(\"Raw data statistics:\")\n",
    "if data.size > 0:\n",
    "    # Print shapes of the first few samples\n",
    "    print(\"Number of samples:\", len(data))\n",
    "    print(\"Shape of sample 0:\", data[0].shape)\n",
    "    # Compute min, max and mean for the first sample (raw frames)\n",
    "    sample0 = data[0]\n",
    "    print(\"Sample 0 - min:\", np.min(sample0), \"max:\", np.max(sample0), \"mean:\", np.mean(sample0))\n",
    "    # Print stddev for all raw data\n",
    "    print(\"Raw data stddev:\", np.std(data))\n",
    "else:\n",
    "    print(\"No data loaded.\")\n",
    "\n",
    "print(\"Label distribution:\")\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "for l, count in zip(unique, counts):\n",
    "    print(f\"Label: {l}, Count: {count}\")\n",
    "\n",
    "print(\"Sequence lengths statistics:\")\n",
    "print(\"Min length:\", np.min(lengths), \"Max length:\", np.max(lengths), \"Mean length:\", np.mean(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb7f9a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after normalization: (3202, 60, 225)\n"
     ]
    }
   ],
   "source": [
    "# Feature normalization (per-sample min-max scaling)\n",
    "def normalize_features(data):\n",
    "    # data: (num_samples, seq_len, num_features)\n",
    "    data_min = data.min(axis=(1, 2), keepdims=True)\n",
    "    data_max = data.max(axis=(1, 2), keepdims=True)\n",
    "    # Avoid division by zero\n",
    "    denom = np.where((data_max - data_min) == 0, 1, data_max - data_min)\n",
    "    normalized = (data - data_min) / denom\n",
    "    return normalized, data_min, data_max\n",
    "\n",
    "data, feat_min, feat_max = normalize_features(data)\n",
    "print(f\"Data shape after normalization: {data.shape}\")\n",
    "\n",
    "def create_mask(lengths, max_len):\n",
    "    # lengths: (num_samples,)\n",
    "    # returns mask: (num_samples, max_len) with 1 for real, 0 for pad\n",
    "    mask = np.zeros((len(lengths), max_len), dtype=np.float32)\n",
    "    for i, l in enumerate(lengths):\n",
    "        mask[i, :l] = 1.0\n",
    "    return mask\n",
    "\n",
    "mask = create_mask(lengths, data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d40e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data statistics:\n",
      "Data shape: (3202, 60, 225)\n",
      "Min value: 0.0 Max value: 1.0 Median value: 0.5581554 Mean value: 0.56632435 Std Dev: 0.13998298\n"
     ]
    }
   ],
   "source": [
    "# After data, feat_center, feat_scale = normalize_features(data)\n",
    "print(\"Normalized data statistics:\")\n",
    "if data.size > 0:\n",
    "    print(\"Data shape:\", data.shape)\n",
    "    print(\"Min value:\", np.min(data), \"Max value:\", np.max(data), \"Median value:\", np.median(data), \"Mean value:\", np.mean(data), \"Std Dev:\", np.std(data))\n",
    "else:\n",
    "    print(\"No data available after normalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60166d",
   "metadata": {},
   "source": [
    "# Dataset Preparation: Train-Test Split\n",
    "\n",
    "Split the dataset into training, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b85c3332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to index mapping: {'about': 0, 'accident': 1, 'africa': 2, 'again': 3, 'all': 4, 'always': 5, 'animal': 6, 'apple': 7, 'approve': 8, 'argue': 9, 'arrive': 10, 'baby': 11, 'back': 12, 'backpack': 13, 'bad': 14, 'bake': 15, 'balance': 16, 'ball': 17, 'banana': 18, 'bar': 19, 'basketball': 20, 'bath': 21, 'bathroom': 22, 'beard': 23, 'because': 24, 'bed': 25, 'before': 26, 'behind': 27, 'bird': 28, 'birthday': 29, 'black': 30, 'blanket': 31, 'blue': 32, 'book': 33, 'bowling': 34, 'boy': 35, 'bring': 36, 'brother': 37, 'brown': 38, 'business': 39, 'but': 40, 'buy': 41, 'call': 42, 'can': 43, 'candy': 44, 'careful': 45, 'cat': 46, 'catch': 47, 'center': 48, 'cereal': 49, 'chair': 50, 'champion': 51, 'change': 52, 'chat': 53, 'cheat': 54, 'check': 55, 'cheese': 56, 'children': 57, 'christmas': 58, 'city': 59, 'class': 60, 'clock': 61, 'close': 62, 'clothes': 63, 'coffee': 64, 'cold': 65, 'college': 66, 'color': 67, 'computer': 68, 'convince': 69, 'cook': 70, 'cool': 71, 'copy': 72, 'corn': 73, 'cough': 74, 'country': 75, 'cousin': 76, 'cow': 77, 'crash': 78, 'crazy': 79, 'cry': 80, 'cute': 81, 'dance': 82, 'dark': 83, 'daughter': 84, 'day': 85, 'deaf': 86, 'decide': 87, 'delay': 88, 'delicious': 89, 'different': 90, 'disappear': 91, 'discuss': 92, 'divorce': 93, 'doctor': 94, 'dog': 95, 'door': 96, 'draw': 97, 'dress': 98, 'drink': 99, 'drive': 100, 'drop': 101, 'east': 102, 'easy': 103, 'eat': 104, 'egg': 105, 'enjoy': 106, 'environment': 107, 'example': 108, 'family': 109, 'far': 110, 'fat': 111, 'father': 112, 'fault': 113, 'feel': 114, 'fine': 115, 'finish': 116, 'first': 117, 'fish': 118, 'flower': 119, 'football': 120, 'forget': 121, 'friend': 122, 'friendly': 123, 'full': 124, 'future': 125, 'game': 126, 'girl': 127, 'give': 128, 'glasses': 129, 'go': 130, 'good': 131, 'government': 132, 'graduate': 133, 'green': 134, 'hair': 135, 'halloween': 136, 'happy': 137, 'hard': 138, 'hat': 139, 'have': 140, 'headache': 141, 'hear': 142, 'hearing': 143, 'heart': 144, 'help': 145, 'here': 146, 'home': 147, 'hope': 148, 'hot': 149, 'hour': 150, 'house': 151, 'how': 152, 'humble': 153, 'hurry': 154, 'husband': 155, 'improve': 156, 'inform': 157, 'interest': 158, 'internet': 159, 'jacket': 160, 'join': 161, 'jump': 162, 'kill': 163, 'kiss': 164, 'knife': 165, 'know': 166, 'language': 167, 'last': 168, 'late': 169, 'later': 170, 'laugh': 171, 'law': 172, 'learn': 173, 'leave': 174, 'letter': 175, 'light': 176, 'like': 177, 'list': 178, 'live': 179, 'lose': 180, 'make': 181, 'man': 182, 'many': 183, 'match': 184, 'mean': 185, 'meat': 186, 'medicine': 187, 'meet': 188, 'milk': 189, 'money': 190, 'more': 191, 'most': 192, 'mother': 193, 'movie': 194, 'music': 195, 'name': 196, 'need': 197, 'new': 198, 'no': 199, 'none': 200, 'now': 201, 'office': 202, 'old': 203, 'orange': 204, 'order': 205, 'paint': 206, 'pants': 207, 'paper': 208, 'party': 209, 'past': 210, 'pencil': 211, 'person': 212, 'pink': 213, 'pizza': 214, 'plan': 215, 'play': 216, 'please': 217, 'police': 218, 'practice': 219, 'president': 220, 'problem': 221, 'pull': 222, 'purple': 223, 'rabbit': 224, 'read': 225, 'red': 226, 'remember': 227, 'restaurant': 228, 'ride': 229, 'right': 230, 'room': 231, 'run': 232, 'russia': 233, 'salt': 234, 'same': 235, 'sandwich': 236, 'school': 237, 'secretary': 238, 'share': 239, 'shirt': 240, 'short': 241, 'show': 242, 'sick': 243, 'sign': 244, 'since': 245, 'small': 246, 'snow': 247, 'some': 248, 'son': 249, 'soon': 250, 'south': 251, 'stay': 252, 'student': 253, 'study': 254, 'sunday': 255, 'table': 256, 'take': 257, 'tall': 258, 'tea': 259, 'teach': 260, 'teacher': 261, 'tell': 262, 'test': 263, 'thanksgiving': 264, 'theory': 265, 'thin': 266, 'thursday': 267, 'time': 268, 'tired': 269, 'tomato': 270, 'trade': 271, 'train': 272, 'travel': 273, 'ugly': 274, 'visit': 275, 'wait': 276, 'walk': 277, 'want': 278, 'war': 279, 'water': 280, 'week': 281, 'what': 282, 'where': 283, 'white': 284, 'who': 285, 'why': 286, 'wife': 287, 'window': 288, 'with': 289, 'woman': 290, 'work': 291, 'write': 292, 'wrong': 293, 'year': 294, 'yellow': 295, 'yes': 296, 'yesterday': 297, 'you': 298, 'your': 299}\n",
      "Converted labels to numeric indices.\n",
      "Training set: (2241, 60, 225), Validation set: (480, 60, 225), Test set: (481, 60, 225)\n",
      "Class weights: tensor([1.3342, 0.8210, 0.8210, 1.0673, 0.8210, 1.1859, 1.0673, 0.8210, 0.9703,\n",
      "        1.0673, 1.0673, 1.0673, 1.5248, 0.9703, 0.9703, 1.3342, 0.9703, 0.9703,\n",
      "        1.0673, 1.0673, 0.8894, 1.0673, 1.0673, 1.0673, 1.5248, 0.8210, 0.6278,\n",
      "        1.1859, 0.8894, 1.1859, 0.8210, 1.3342, 0.8894, 1.0673, 0.8210, 1.0673,\n",
      "        1.0673, 0.9703, 0.9703, 0.9703, 1.0673, 0.9703, 0.9703, 0.8894, 0.7624,\n",
      "        1.3342, 0.9703, 1.1859, 1.0673, 1.1859, 1.1859, 1.3342, 0.8894, 1.0673,\n",
      "        0.8894, 0.8210, 1.1859, 1.3342, 1.1859, 0.8210, 1.0673, 1.5248, 1.3342,\n",
      "        1.1859, 1.1859, 0.8894, 0.9703, 0.7624, 0.5337, 0.9703, 0.8894, 0.8210,\n",
      "        1.1859, 0.8894, 1.1859, 1.0673, 0.7116, 0.8210, 0.9703, 1.1859, 1.0673,\n",
      "        1.3342, 0.9703, 0.8210, 0.9703, 1.0673, 0.8210, 0.9703, 1.1859, 1.0673,\n",
      "        0.9703, 1.1859, 0.9703, 1.5248, 1.0673, 0.7624, 1.0673, 1.0673, 1.1859,\n",
      "        0.5083, 1.3342, 1.0673, 1.3342, 1.3342, 1.1859, 1.3342, 1.0673, 1.1859,\n",
      "        1.3342, 0.7624, 0.8894, 0.9703, 1.0673, 1.5248, 1.1859, 0.7624, 0.8894,\n",
      "        1.1859, 0.7624, 1.1859, 1.1859, 0.7624, 1.3342, 1.3342, 0.8894, 1.1859,\n",
      "        1.1859, 1.0673, 0.8210, 1.0673, 0.6278, 1.0673, 0.9703, 0.8894, 1.0673,\n",
      "        0.7624, 1.1859, 1.0673, 1.5248, 0.8210, 1.3342, 0.8210, 1.0673, 0.7624,\n",
      "        1.1859, 0.7624, 1.1859, 1.1859, 0.9703, 0.7116, 1.1859, 1.1859, 1.1859,\n",
      "        1.0673, 1.5248, 1.1859, 1.1859, 1.1859, 0.8894, 1.0673, 1.0673, 1.0673,\n",
      "        1.3342, 1.1859, 0.9703, 1.7789, 0.9703, 0.8210, 0.8894, 1.0673, 0.8210,\n",
      "        0.8894, 1.0673, 0.9703, 0.8894, 0.8894, 0.9703, 0.6278, 0.8894, 1.7789,\n",
      "        0.8894, 1.1859, 0.8210, 0.8210, 1.0673, 1.3342, 1.1859, 1.1859, 0.8210,\n",
      "        1.1859, 1.3342, 1.1859, 1.0673, 0.6671, 1.5248, 1.0673, 0.9703, 0.9703,\n",
      "        1.0673, 0.7116, 1.3342, 0.7624, 1.0673, 1.3342, 0.7624, 1.3342, 0.9703,\n",
      "        1.0673, 0.8894, 1.1859, 1.0673, 1.5248, 1.0673, 0.8894, 0.7116, 1.1859,\n",
      "        0.8210, 1.3342, 1.0673, 1.3342, 0.9703, 1.1859, 0.8894, 0.9703, 0.9703,\n",
      "        1.1859, 1.1859, 1.0673, 1.3342, 1.3342, 0.8894, 1.0673, 1.0673, 0.9703,\n",
      "        0.9703, 0.8894, 0.9703, 0.9703, 0.8210, 1.0673, 0.7624, 0.8210, 1.0673,\n",
      "        0.9703, 1.1859, 1.1859, 1.0673, 1.3342, 1.1859, 0.8894, 1.0673, 1.0673,\n",
      "        1.1859, 0.9703, 0.7116, 0.9703, 1.3342, 0.9703, 0.7624, 1.1859, 1.0673,\n",
      "        0.8894, 0.8894, 1.3342, 0.7116, 1.1859, 0.6671, 0.8210, 1.0673, 1.0673,\n",
      "        1.3342, 0.9703, 1.1859, 1.0673, 0.9703, 0.8894, 0.8210, 0.9703, 0.9703,\n",
      "        1.1859, 0.8894, 0.9703, 1.0673, 1.1859, 0.8210, 0.7116, 1.3342, 0.9703,\n",
      "        1.3342, 1.0673, 0.8210, 0.8894, 0.8894, 0.8894, 0.8210, 0.9703, 0.7116,\n",
      "        0.8894, 0.8894, 0.9703], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split if not already imported\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# Create a mapping from string labels to numeric indices\n",
    "label_to_index = {label: idx for idx, label in enumerate(np.unique(labels))}\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "print(f\"Label to index mapping: {label_to_index}\")\n",
    "\n",
    "# Convert string labels to numeric indices\n",
    "numeric_labels = np.array([label_to_index[label] for label in labels])\n",
    "print(f\"Converted labels to numeric indices.\")\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_temp, y_train, y_temp, mask_train, mask_temp = train_test_split(data, numeric_labels, mask, test_size=0.3, stratify=numeric_labels, random_state=42)\n",
    "X_val, X_test, y_val, y_test, mask_val, mask_test = train_test_split(X_temp, y_temp, mask_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "# Compute class weights for balancing\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(numeric_labels), y=numeric_labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(f\"Class weights: {class_weights}\")\n",
    " \n",
    "# Update DataLoader to use weighted sampling\n",
    "# NOTE: train_dataset is not defined yet here, so just define the sampler and sample_weights for later use\n",
    "sample_weights = class_weights[numeric_labels]\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "# Do not create train_loader here; it will be created after train_dataset is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e866d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class ASLDataset(Dataset):\n",
    "    def __init__(self, data, labels, mask):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx], self.mask[idx]\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = ASLDataset(X_train, y_train, mask_train)\n",
    "val_dataset = ASLDataset(X_val, y_val, mask_val)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb276e",
   "metadata": {},
   "source": [
    "# Model Definition: BiLSTM for Temporal Data\n",
    "\n",
    "Define a BiLSTM model for ASL recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95231d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (3202, 60, 225)\n",
      "Each sample has 60 frames with 225 features per frame\n",
      "Input size for the model: 225\n",
      "Number of unique labels (classes): 300\n",
      "BiLSTM(\n",
      "  (lstm): LSTM(225, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=300, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Print information about the data shape\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "if len(data) > 0:\n",
    "    print(f\"Each sample has {data[0].shape[0]} frames with {data[0].shape[1]} features per frame\")\n",
    "\n",
    "# Replace RNNModel with BiLSTM\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout_rate=0.3):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout_rate if num_layers > 1 else 0\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (batch, seq, feat)\n",
    "        # mask: (batch, seq)\n",
    "        out, _ = self.lstm(x)  # (batch, seq, hidden*2)\n",
    "        if mask is not None:\n",
    "            # For each sample, get the last valid (unpadded) output\n",
    "            lengths = mask.sum(dim=1).long()  # (batch,)\n",
    "            last_outputs = []\n",
    "            for i, l in enumerate(lengths):\n",
    "                last_outputs.append(out[i, l-1, :])\n",
    "            out = torch.stack(last_outputs, dim=0)  # (batch, hidden*2)\n",
    "        else:\n",
    "            out = out[:, -1, :]\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Get the input size from the processed data\n",
    "input_size = data[0].shape[1] if len(data) > 0 else 0\n",
    "hidden_size = 128\n",
    "output_size = len(np.unique(labels))\n",
    "print(f\"Input size for the model: {input_size}\")\n",
    "print(f\"Number of unique labels (classes): {output_size}\")\n",
    "\n",
    "model = BiLSTM(input_size, hidden_size, output_size)\n",
    "print(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9840ccc",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Grid Search\n",
    "\n",
    "We use a custom grid search implementation for hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "032d4807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-19 23:05:40,656] A new study created in memory with name: no-name-525074af-57bb-484f-9af0-c6429ef5cd7e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization using Optuna...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zikru\\AppData\\Local\\Temp\\ipykernel_27168\\3935501238.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\zikru\\AppData\\Local\\Temp\\ipykernel_27168\\3935501238.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.8)\n",
      "C:\\Users\\zikru\\AppData\\Local\\Temp\\ipykernel_27168\\3935501238.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-4, 1e-2)\n",
      "[I 2025-05-19 23:05:57,387] Trial 0 finished with value: 8.004262129465738 and parameters: {'learning_rate': 0.0836391381522659, 'hidden_size': 128, 'dropout_rate': 0.24920083011100747, 'batch_size': 16, 'weight_decay': 0.00020274495282427478, 'num_layers': 1}. Best is trial 0 with value: 8.004262129465738.\n",
      "[I 2025-05-19 23:05:57,387] Trial 0 finished with value: 8.004262129465738 and parameters: {'learning_rate': 0.0836391381522659, 'hidden_size': 128, 'dropout_rate': 0.24920083011100747, 'batch_size': 16, 'weight_decay': 0.00020274495282427478, 'num_layers': 1}. Best is trial 0 with value: 8.004262129465738.\n",
      "C:\\Users\\zikru\\AppData\\Local\\Temp\\ipykernel_27168\\3935501238.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\zikru\\AppData\\Local\\Temp\\ipykernel_27168\\3935501238.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.8)\n",
      "C:\\Users\\zikru\\AppData\\Local\\Temp\\ipykernel_27168\\3935501238.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-4, 1e-2)\n",
      "C:\\Users\\zikru\\AppData\\Local\\Temp\\ipykernel_27168\\3935501238.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\zikru\\AppData\\Local\\Temp\\ipykernel_27168\\3935501238.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.8)\n",
      "C:\\Users\\zikru\\AppData\\Local\\Temp\\ipykernel_27168\\3935501238.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-4, 1e-2)\n",
      "[I 2025-05-19 23:06:14,877] Trial 1 finished with value: 5.7090353012084964 and parameters: {'learning_rate': 0.0004303377097350338, 'hidden_size': 64, 'dropout_rate': 0.4809509772468083, 'batch_size': 16, 'weight_decay': 0.000139396230162287, 'num_layers': 3}. Best is trial 1 with value: 5.7090353012084964.\n",
      "[I 2025-05-19 23:06:14,877] Trial 1 finished with value: 5.7090353012084964 and parameters: {'learning_rate': 0.0004303377097350338, 'hidden_size': 64, 'dropout_rate': 0.4809509772468083, 'batch_size': 16, 'weight_decay': 0.000139396230162287, 'num_layers': 3}. Best is trial 1 with value: 5.7090353012084964.\n",
      "[I 2025-05-19 23:06:25,262] Trial 2 finished with value: 5.7134170214335125 and parameters: {'learning_rate': 0.0024364463335954413, 'hidden_size': 128, 'dropout_rate': 0.2562320333176145, 'batch_size': 32, 'weight_decay': 0.003372115241864127, 'num_layers': 2}. Best is trial 1 with value: 5.7090353012084964.\n",
      "[I 2025-05-19 23:06:25,262] Trial 2 finished with value: 5.7134170214335125 and parameters: {'learning_rate': 0.0024364463335954413, 'hidden_size': 128, 'dropout_rate': 0.2562320333176145, 'batch_size': 32, 'weight_decay': 0.003372115241864127, 'num_layers': 2}. Best is trial 1 with value: 5.7090353012084964.\n",
      "[I 2025-05-19 23:06:37,963] Trial 3 finished with value: 5.710096788406372 and parameters: {'learning_rate': 0.0008153884468632813, 'hidden_size': 128, 'dropout_rate': 0.4938792369866846, 'batch_size': 16, 'weight_decay': 0.0073923343378563275, 'num_layers': 1}. Best is trial 1 with value: 5.7090353012084964.\n",
      "[I 2025-05-19 23:06:37,963] Trial 3 finished with value: 5.710096788406372 and parameters: {'learning_rate': 0.0008153884468632813, 'hidden_size': 128, 'dropout_rate': 0.4938792369866846, 'batch_size': 16, 'weight_decay': 0.0073923343378563275, 'num_layers': 1}. Best is trial 1 with value: 5.7090353012084964.\n",
      "[I 2025-05-19 23:06:47,768] Trial 4 finished with value: 5.7525303840637205 and parameters: {'learning_rate': 0.05352161880124403, 'hidden_size': 64, 'dropout_rate': 0.6256753986115835, 'batch_size': 32, 'weight_decay': 0.005894980994973556, 'num_layers': 2}. Best is trial 1 with value: 5.7090353012084964.\n",
      "[I 2025-05-19 23:06:47,768] Trial 4 finished with value: 5.7525303840637205 and parameters: {'learning_rate': 0.05352161880124403, 'hidden_size': 64, 'dropout_rate': 0.6256753986115835, 'batch_size': 32, 'weight_decay': 0.005894980994973556, 'num_layers': 2}. Best is trial 1 with value: 5.7090353012084964.\n",
      "[I 2025-05-19 23:06:58,030] Trial 5 finished with value: 5.706318187713623 and parameters: {'learning_rate': 0.0022818287736223433, 'hidden_size': 32, 'dropout_rate': 0.3647233806073735, 'batch_size': 32, 'weight_decay': 0.004327172888931149, 'num_layers': 2}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:06:58,030] Trial 5 finished with value: 5.706318187713623 and parameters: {'learning_rate': 0.0022818287736223433, 'hidden_size': 32, 'dropout_rate': 0.3647233806073735, 'batch_size': 32, 'weight_decay': 0.004327172888931149, 'num_layers': 2}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:06:59,725] Trial 6 pruned. \n",
      "[I 2025-05-19 23:06:59,725] Trial 6 pruned. \n",
      "[I 2025-05-19 23:07:01,086] Trial 7 pruned. \n",
      "[I 2025-05-19 23:07:01,086] Trial 7 pruned. \n",
      "[I 2025-05-19 23:07:02,655] Trial 8 pruned. \n",
      "[I 2025-05-19 23:07:02,655] Trial 8 pruned. \n",
      "[I 2025-05-19 23:07:04,079] Trial 9 pruned. \n",
      "[I 2025-05-19 23:07:04,079] Trial 9 pruned. \n",
      "[I 2025-05-19 23:07:04,877] Trial 10 pruned. \n",
      "[I 2025-05-19 23:07:04,877] Trial 10 pruned. \n",
      "[I 2025-05-19 23:07:05,981] Trial 11 pruned. \n",
      "[I 2025-05-19 23:07:05,981] Trial 11 pruned. \n",
      "[I 2025-05-19 23:07:06,767] Trial 12 pruned. \n",
      "[I 2025-05-19 23:07:06,767] Trial 12 pruned. \n",
      "[I 2025-05-19 23:07:07,771] Trial 13 pruned. \n",
      "[I 2025-05-19 23:07:07,771] Trial 13 pruned. \n",
      "[I 2025-05-19 23:07:18,188] Trial 14 finished with value: 5.712078698476156 and parameters: {'learning_rate': 0.0012233753199975563, 'hidden_size': 32, 'dropout_rate': 0.31219822984161416, 'batch_size': 32, 'weight_decay': 0.00034365923993900775, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:07:18,188] Trial 14 finished with value: 5.712078698476156 and parameters: {'learning_rate': 0.0012233753199975563, 'hidden_size': 32, 'dropout_rate': 0.31219822984161416, 'batch_size': 32, 'weight_decay': 0.00034365923993900775, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:07:18,867] Trial 15 pruned. \n",
      "[I 2025-05-19 23:07:18,867] Trial 15 pruned. \n",
      "[I 2025-05-19 23:07:29,871] Trial 16 finished with value: 5.711807854970297 and parameters: {'learning_rate': 0.00040717591493112074, 'hidden_size': 64, 'dropout_rate': 0.5285047002832286, 'batch_size': 32, 'weight_decay': 0.00443207643681945, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:07:29,871] Trial 16 finished with value: 5.711807854970297 and parameters: {'learning_rate': 0.00040717591493112074, 'hidden_size': 64, 'dropout_rate': 0.5285047002832286, 'batch_size': 32, 'weight_decay': 0.00443207643681945, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:07:30,942] Trial 17 pruned. \n",
      "[I 2025-05-19 23:07:30,942] Trial 17 pruned. \n",
      "[I 2025-05-19 23:07:32,441] Trial 18 pruned. \n",
      "[I 2025-05-19 23:07:32,441] Trial 18 pruned. \n",
      "[I 2025-05-19 23:07:33,193] Trial 19 pruned. \n",
      "[I 2025-05-19 23:07:33,193] Trial 19 pruned. \n",
      "[I 2025-05-19 23:07:44,018] Trial 20 finished with value: 5.707917372385661 and parameters: {'learning_rate': 0.00901684242839607, 'hidden_size': 32, 'dropout_rate': 0.6686795234902676, 'batch_size': 32, 'weight_decay': 0.0013354003587034429, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:07:44,018] Trial 20 finished with value: 5.707917372385661 and parameters: {'learning_rate': 0.00901684242839607, 'hidden_size': 32, 'dropout_rate': 0.6686795234902676, 'batch_size': 32, 'weight_decay': 0.0013354003587034429, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:07:54,749] Trial 21 finished with value: 5.706674798329671 and parameters: {'learning_rate': 0.007001507497042739, 'hidden_size': 32, 'dropout_rate': 0.6753325402641207, 'batch_size': 32, 'weight_decay': 0.0017980594943147726, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:07:54,749] Trial 21 finished with value: 5.706674798329671 and parameters: {'learning_rate': 0.007001507497042739, 'hidden_size': 32, 'dropout_rate': 0.6753325402641207, 'batch_size': 32, 'weight_decay': 0.0017980594943147726, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:07:55,833] Trial 22 pruned. \n",
      "[I 2025-05-19 23:07:55,833] Trial 22 pruned. \n",
      "[I 2025-05-19 23:08:06,117] Trial 23 finished with value: 5.708316739400228 and parameters: {'learning_rate': 0.02278182357982261, 'hidden_size': 32, 'dropout_rate': 0.7998320002090774, 'batch_size': 32, 'weight_decay': 0.004210359901085137, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:08:06,117] Trial 23 finished with value: 5.708316739400228 and parameters: {'learning_rate': 0.02278182357982261, 'hidden_size': 32, 'dropout_rate': 0.7998320002090774, 'batch_size': 32, 'weight_decay': 0.004210359901085137, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:08:15,590] Trial 24 finished with value: 5.708482646942139 and parameters: {'learning_rate': 0.006775627545861005, 'hidden_size': 32, 'dropout_rate': 0.6746146468387386, 'batch_size': 32, 'weight_decay': 0.0017586011508104675, 'num_layers': 2}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:08:15,590] Trial 24 finished with value: 5.708482646942139 and parameters: {'learning_rate': 0.006775627545861005, 'hidden_size': 32, 'dropout_rate': 0.6746146468387386, 'batch_size': 32, 'weight_decay': 0.0017586011508104675, 'num_layers': 2}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:08:16,614] Trial 25 pruned. \n",
      "[I 2025-05-19 23:08:16,614] Trial 25 pruned. \n",
      "[I 2025-05-19 23:08:18,658] Trial 26 pruned. \n",
      "[I 2025-05-19 23:08:18,658] Trial 26 pruned. \n",
      "[I 2025-05-19 23:08:19,644] Trial 27 pruned. \n",
      "[I 2025-05-19 23:08:19,644] Trial 27 pruned. \n",
      "[I 2025-05-19 23:08:20,636] Trial 28 pruned. \n",
      "[I 2025-05-19 23:08:20,636] Trial 28 pruned. \n",
      "[I 2025-05-19 23:08:21,578] Trial 29 pruned. \n",
      "[I 2025-05-19 23:08:21,578] Trial 29 pruned. \n",
      "[I 2025-05-19 23:08:22,582] Trial 30 pruned. \n",
      "[I 2025-05-19 23:08:22,582] Trial 30 pruned. \n",
      "[I 2025-05-19 23:08:34,094] Trial 31 finished with value: 5.710210704803467 and parameters: {'learning_rate': 0.022596121382652008, 'hidden_size': 32, 'dropout_rate': 0.7813126858175365, 'batch_size': 32, 'weight_decay': 0.004088326454025754, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:08:34,094] Trial 31 finished with value: 5.710210704803467 and parameters: {'learning_rate': 0.022596121382652008, 'hidden_size': 32, 'dropout_rate': 0.7813126858175365, 'batch_size': 32, 'weight_decay': 0.004088326454025754, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:08:45,460] Trial 32 pruned. \n",
      "[I 2025-05-19 23:08:45,460] Trial 32 pruned. \n",
      "[I 2025-05-19 23:08:46,614] Trial 33 pruned. \n",
      "[I 2025-05-19 23:08:46,614] Trial 33 pruned. \n",
      "[I 2025-05-19 23:08:47,636] Trial 34 pruned. \n",
      "[I 2025-05-19 23:08:47,636] Trial 34 pruned. \n",
      "[I 2025-05-19 23:08:48,660] Trial 35 pruned. \n",
      "[I 2025-05-19 23:08:48,660] Trial 35 pruned. \n",
      "[I 2025-05-19 23:08:50,098] Trial 36 pruned. \n",
      "[I 2025-05-19 23:08:50,098] Trial 36 pruned. \n",
      "[I 2025-05-19 23:08:50,887] Trial 37 pruned. \n",
      "[I 2025-05-19 23:08:50,887] Trial 37 pruned. \n",
      "[I 2025-05-19 23:08:51,749] Trial 38 pruned. \n",
      "[I 2025-05-19 23:08:51,749] Trial 38 pruned. \n",
      "[I 2025-05-19 23:08:53,932] Trial 39 pruned. \n",
      "[I 2025-05-19 23:08:53,932] Trial 39 pruned. \n",
      "[I 2025-05-19 23:08:55,617] Trial 40 pruned. \n",
      "[I 2025-05-19 23:08:55,617] Trial 40 pruned. \n",
      "[I 2025-05-19 23:08:56,856] Trial 41 pruned. \n",
      "[I 2025-05-19 23:08:56,856] Trial 41 pruned. \n",
      "[I 2025-05-19 23:08:58,070] Trial 42 pruned. \n",
      "[I 2025-05-19 23:08:58,070] Trial 42 pruned. \n",
      "[I 2025-05-19 23:09:01,309] Trial 43 pruned. \n",
      "[I 2025-05-19 23:09:01,309] Trial 43 pruned. \n",
      "[I 2025-05-19 23:09:03,327] Trial 44 pruned. \n",
      "[I 2025-05-19 23:09:03,327] Trial 44 pruned. \n",
      "[I 2025-05-19 23:09:07,084] Trial 45 pruned. \n",
      "[I 2025-05-19 23:09:07,084] Trial 45 pruned. \n",
      "[I 2025-05-19 23:09:31,926] Trial 46 finished with value: 5.707933855056763 and parameters: {'learning_rate': 0.01252537597073213, 'hidden_size': 32, 'dropout_rate': 0.7610598346380112, 'batch_size': 16, 'weight_decay': 0.0022969848976912466, 'num_layers': 2}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:09:31,926] Trial 46 finished with value: 5.707933855056763 and parameters: {'learning_rate': 0.01252537597073213, 'hidden_size': 32, 'dropout_rate': 0.7610598346380112, 'batch_size': 16, 'weight_decay': 0.0022969848976912466, 'num_layers': 2}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:10:03,335] Trial 47 finished with value: 5.7083738327026365 and parameters: {'learning_rate': 0.012803507496666203, 'hidden_size': 32, 'dropout_rate': 0.7578265172391836, 'batch_size': 16, 'weight_decay': 0.002572308985427061, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:10:03,335] Trial 47 finished with value: 5.7083738327026365 and parameters: {'learning_rate': 0.012803507496666203, 'hidden_size': 32, 'dropout_rate': 0.7578265172391836, 'batch_size': 16, 'weight_decay': 0.002572308985427061, 'num_layers': 3}. Best is trial 5 with value: 5.706318187713623.\n",
      "[I 2025-05-19 23:10:05,648] Trial 48 pruned. \n",
      "[I 2025-05-19 23:10:05,648] Trial 48 pruned. \n",
      "[I 2025-05-19 23:10:07,352] Trial 49 pruned. \n",
      "[I 2025-05-19 23:10:07,352] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.0022818287736223433, 'hidden_size': 32, 'dropout_rate': 0.3647233806073735, 'batch_size': 32, 'weight_decay': 0.004327172888931149, 'num_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "# Import Optuna for hyperparameter optimization\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "print(\"Starting hyperparameter optimization using Optuna...\")\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [32, 64, 128])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.8)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-4, 1e-2)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "\n",
    "    # Create the model with the sampled hyperparameters\n",
    "    model = BiLSTM(input_size, hidden_size, output_size, num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Training loop (simplified for Optuna)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(10):  # Limit epochs for faster optimization\n",
    "        model.train()\n",
    "        for inputs, targets, mask in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, mask)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, mask in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs, mask)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Report validation loss to Optuna\n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "        # Handle pruning\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, timeout=3600)\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Ensure all required parameters are stored for model training\n",
    "best_params = {\n",
    "    'learning_rate': best_params['learning_rate'],\n",
    "    'hidden_size': best_params['hidden_size'],\n",
    "    'dropout_rate': best_params['dropout_rate'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'weight_decay': best_params['weight_decay'],\n",
    "    'num_layers': best_params['num_layers']\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17410aea",
   "metadata": {},
   "source": [
    "# Model Training with K-Fold Cross-Validation\n",
    "\n",
    "Train the model using k-fold cross-validation to better evaluate its performance. This section also switches to an RNN-based architecture and ensures proper batch normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b730f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.notebook import tqdm  # Use notebook-friendly version of tqdm\n",
    "\n",
    "# Define the RNN model for classification\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout_rate=0.3):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size, hidden_size, batch_first=True, num_layers=num_layers, dropout=dropout_rate if num_layers > 1 else 0\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        # Dropout after RNN output\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.layer_norm(x[:, -1, :])\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Use best hyperparameters from grid search\n",
    "best_lr = best_params['learning_rate']\n",
    "best_hidden_size = best_params['hidden_size']\n",
    "best_dropout_rate = best_params['dropout_rate']\n",
    "best_batch_size = max(2, best_params['batch_size'])  # Ensure batch size is above 1\n",
    "best_weight_decay = best_params['weight_decay']\n",
    "best_num_layers = best_params['num_layers']\n",
    "\n",
    "print(f\"Training with best hyperparameters:\")\n",
    "print(f\"  Learning rate: {best_lr}\")\n",
    "print(f\"  Hidden size: {best_hidden_size}\")\n",
    "print(f\"  Dropout rate: {best_dropout_rate}\")\n",
    "print(f\"  Batch size: {best_batch_size}\")\n",
    "print(f\"  Weight decay: {best_weight_decay}\")\n",
    "print(f\"  Number of layers: {best_num_layers}\")\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "k_folds = 3\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Training with k-fold cross-validation\n",
    "fold_results = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
    "    print(f\"Fold {fold+1}/{k_folds}\")\n",
    "\n",
    "    # Create data loaders for the current fold\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    val_subset = Subset(train_dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=best_batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "    # Recreate model for each fold with best hyperparameters\n",
    "    model = BiLSTM(\n",
    "        input_size, best_hidden_size, output_size, num_layers=best_num_layers, dropout_rate=best_dropout_rate).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer with weight decay\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n",
    "    # Use ReduceLROnPlateau scheduler for better learning rate adaptation\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    patience = 10\n",
    "    early_stop_counter = 0\n",
    "    best_val_loss = float('inf')\n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\", end=\"\\r\")\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training\", leave=False):\n",
    "            inputs, targets, mask = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, mask)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Validation\", leave=False):\n",
    "                inputs, targets, mask = batch\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs, mask)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Print progress only once per epoch\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save the model if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"best_model_fold{fold+1}.pth\")\n",
    "            print(\"Model checkpoint saved!\")\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"Early stopping counter: {early_stop_counter}/{patience}\")\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    fold_results.append(best_val_loss)\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"\\nCross-validation results:\")\n",
    "for fold, loss in enumerate(fold_results):\n",
    "    print(f\"Fold {fold+1}: Validation Loss = {loss:.4f}\")\n",
    "print(f\"Average Validation Loss: {sum(fold_results)/len(fold_results):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Loss Curves\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curves\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7bf6d",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Evaluate the model on the test set and display metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_dataset = ASLDataset(X_test, y_test, mask_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, targets, mask = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move inputs to device too\n",
    "        outputs = model(inputs, mask)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "test_loss /= len(test_loader)\n",
    "accuracy = correct / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the model for transfer learning\n",
    "os.makedirs('./models', exist_ok=True)  # Create directory if it doesn't exist\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'input_size': input_size,\n",
    "    'hidden_size': best_hidden_size,\n",
    "    'dropout_rate': best_dropout_rate,\n",
    "    'output_size': output_size,\n",
    "    'accuracy': accuracy\n",
    "}, \"./models/transfer_learning_model.pth\")\n",
    "print(\"Model saved for transfer learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90776ea",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Test the model on new data or perform real-time inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb18cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "model.eval()\n",
    "sample = torch.tensor(X_test[0:1], dtype=torch.float32).to(device)  # Move sample to the correct device\n",
    "with torch.no_grad():\n",
    "    prediction = model(sample)\n",
    "    predicted_label = torch.argmax(prediction, dim=1).item()\n",
    "print(f\"Predicted label: {predicted_label}, True label: {y_test[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aslpose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
