{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db43fcdf",
   "metadata": {},
   "source": [
    "# ASL Recognition: End-to-End Workflow\n",
    "\n",
    "This notebook consolidates the entire workflow for preprocessing, model training, and evaluation for American Sign Language (ASL) recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a040ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm  # Use notebook-friendly version of tqdm\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b7725",
   "metadata": {},
   "source": [
    "# Preprocessing: Load and Normalize Keypoints\n",
    "\n",
    "We will load the preprocessed keypoints from the JSON files and normalize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4673b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading keypoints from f:\\Uni_Stuff\\6th_Sem\\DL\\Proj\\video-asl-recognition\\pose_estimation\\data\\keypoints\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136492f2bf9747089fc63545395ef36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading keypoint files:   0%|          | 0/3202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3202 files successfully, skipped 0 files\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(r'f:\\Uni_Stuff\\6th_Sem\\DL\\Proj\\video-asl-recognition\\pose_estimation\\data\\keypoints')\n",
    "\n",
    "# Load and normalize keypoints\n",
    "# Now also return sequence lengths for masking\n",
    "\n",
    "def load_keypoints(data_dir):\n",
    "    data = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    skipped_files = 0\n",
    "    processed_files = 0\n",
    "    print(f\"Loading keypoints from {data_dir}\")\n",
    "    # Gather all json files from all label directories\n",
    "    all_json_files = []\n",
    "    for label_dir in data_dir.iterdir():\n",
    "        if label_dir.is_dir():\n",
    "            all_json_files.extend(list(label_dir.glob('*.json')))\n",
    "    for json_file in tqdm(all_json_files, desc=\"Loading keypoint files\"):\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                if 'keypoints' not in content or 'label' not in content:\n",
    "                    skipped_files += 1\n",
    "                    continue\n",
    "                keypoints = content['keypoints']\n",
    "                label = content['label']\n",
    "                processed_frames = []\n",
    "                for frame in keypoints:\n",
    "                    left_hand_features = np.zeros(63, dtype=np.float32)\n",
    "                    right_hand_features = np.zeros(63, dtype=np.float32)\n",
    "                    pose_features = np.zeros(99, dtype=np.float32)\n",
    "                    if 'hands' in frame and frame['hands']:\n",
    "                        for i, hand in enumerate(frame['hands']):\n",
    "                            if i < 2:\n",
    "                                hand_features = []\n",
    "                                for point in hand:\n",
    "                                    if isinstance(point, list) and len(point) == 3:\n",
    "                                        hand_features.extend(point)\n",
    "                                if i == 0 and len(hand_features) <= 63:\n",
    "                                    left_hand_features[:len(hand_features)] = hand_features\n",
    "                                elif i == 1 and len(hand_features) <= 63:\n",
    "                                    right_hand_features[:len(hand_features)] = hand_features\n",
    "                    if 'pose' in frame and frame['pose']:\n",
    "                        pose_data = []\n",
    "                        for point in frame['pose']:\n",
    "                            if isinstance(point, list) and len(point) == 3:\n",
    "                                pose_data.extend(point)\n",
    "                        if len(pose_data) <= 99:\n",
    "                            pose_features[:len(pose_data)] = pose_data\n",
    "                    frame_features = np.concatenate([left_hand_features, right_hand_features, pose_features])\n",
    "                    processed_frames.append(frame_features)\n",
    "                if processed_frames:\n",
    "                    processed_data = np.array(processed_frames, dtype=np.float32)\n",
    "                    if processed_data.shape[0] > 0 and processed_data.shape[1] > 0:\n",
    "                        data.append(processed_data)\n",
    "                        labels.append(label)\n",
    "                        lengths.append(processed_data.shape[0])\n",
    "                        processed_files += 1\n",
    "                    else:\n",
    "                        skipped_files += 1\n",
    "                else:\n",
    "                    skipped_files += 1\n",
    "        except Exception as e:\n",
    "            skipped_files += 1\n",
    "    print(f\"Processed {processed_files} files successfully, skipped {skipped_files} files\")\n",
    "    if not data:\n",
    "        print(\"Warning: No valid data was loaded!\")\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "    max_seq_len = max(sample.shape[0] for sample in data)\n",
    "    feature_dim = data[0].shape[1]\n",
    "    padded_data = []\n",
    "    for sample in data:\n",
    "        if sample.shape[0] < max_seq_len:\n",
    "            padding = np.zeros((max_seq_len - sample.shape[0], feature_dim), dtype=np.float32)\n",
    "            padded_sample = np.vstack((sample, padding))\n",
    "        else:\n",
    "            padded_sample = sample[:max_seq_len]\n",
    "        padded_data.append(padded_sample)\n",
    "    return np.array(padded_data), np.array(labels), np.array(lengths)\n",
    "\n",
    "data, labels, lengths = load_keypoints(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a50aa2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 labels:\n",
      "['about' 'about' 'about' 'about' 'about']\n",
      "First 5 lengths:\n",
      "[60 60 60 60 60]\n",
      "Raw data statistics:\n",
      "Number of samples: 3202\n",
      "Shape of sample 0: (60, 225)\n",
      "Sample 0 - min: -2.6472163 max: 2.4988759 mean: 0.23723178\n",
      "Label distribution:\n",
      "Label: about, Count: 8\n",
      "Label: accident, Count: 13\n",
      "Label: africa, Count: 13\n",
      "Label: again, Count: 10\n",
      "Label: all, Count: 13\n",
      "Label: always, Count: 9\n",
      "Label: animal, Count: 10\n",
      "Label: apple, Count: 13\n",
      "Label: approve, Count: 11\n",
      "Label: argue, Count: 10\n",
      "Label: arrive, Count: 10\n",
      "Label: baby, Count: 10\n",
      "Label: back, Count: 7\n",
      "Label: backpack, Count: 11\n",
      "Label: bad, Count: 11\n",
      "Label: bake, Count: 8\n",
      "Label: balance, Count: 11\n",
      "Label: ball, Count: 11\n",
      "Label: banana, Count: 10\n",
      "Label: bar, Count: 10\n",
      "Label: basketball, Count: 12\n",
      "Label: bath, Count: 10\n",
      "Label: bathroom, Count: 10\n",
      "Label: beard, Count: 10\n",
      "Label: because, Count: 7\n",
      "Label: bed, Count: 13\n",
      "Label: before, Count: 17\n",
      "Label: behind, Count: 9\n",
      "Label: bird, Count: 12\n",
      "Label: birthday, Count: 9\n",
      "Label: black, Count: 13\n",
      "Label: blanket, Count: 8\n",
      "Label: blue, Count: 12\n",
      "Label: book, Count: 10\n",
      "Label: bowling, Count: 13\n",
      "Label: boy, Count: 10\n",
      "Label: bring, Count: 10\n",
      "Label: brother, Count: 11\n",
      "Label: brown, Count: 11\n",
      "Label: business, Count: 11\n",
      "Label: but, Count: 10\n",
      "Label: buy, Count: 11\n",
      "Label: call, Count: 11\n",
      "Label: can, Count: 12\n",
      "Label: candy, Count: 14\n",
      "Label: careful, Count: 8\n",
      "Label: cat, Count: 11\n",
      "Label: catch, Count: 9\n",
      "Label: center, Count: 10\n",
      "Label: cereal, Count: 9\n",
      "Label: chair, Count: 9\n",
      "Label: champion, Count: 8\n",
      "Label: change, Count: 12\n",
      "Label: chat, Count: 10\n",
      "Label: cheat, Count: 12\n",
      "Label: check, Count: 13\n",
      "Label: cheese, Count: 9\n",
      "Label: children, Count: 8\n",
      "Label: christmas, Count: 9\n",
      "Label: city, Count: 13\n",
      "Label: class, Count: 10\n",
      "Label: clock, Count: 7\n",
      "Label: close, Count: 8\n",
      "Label: clothes, Count: 9\n",
      "Label: coffee, Count: 9\n",
      "Label: cold, Count: 12\n",
      "Label: college, Count: 11\n",
      "Label: color, Count: 14\n",
      "Label: computer, Count: 20\n",
      "Label: convince, Count: 11\n",
      "Label: cook, Count: 12\n",
      "Label: cool, Count: 13\n",
      "Label: copy, Count: 9\n",
      "Label: corn, Count: 12\n",
      "Label: cough, Count: 9\n",
      "Label: country, Count: 10\n",
      "Label: cousin, Count: 15\n",
      "Label: cow, Count: 13\n",
      "Label: crash, Count: 11\n",
      "Label: crazy, Count: 9\n",
      "Label: cry, Count: 10\n",
      "Label: cute, Count: 8\n",
      "Label: dance, Count: 11\n",
      "Label: dark, Count: 13\n",
      "Label: daughter, Count: 11\n",
      "Label: day, Count: 10\n",
      "Label: deaf, Count: 13\n",
      "Label: decide, Count: 11\n",
      "Label: delay, Count: 9\n",
      "Label: delicious, Count: 10\n",
      "Label: different, Count: 11\n",
      "Label: disappear, Count: 9\n",
      "Label: discuss, Count: 11\n",
      "Label: divorce, Count: 7\n",
      "Label: doctor, Count: 10\n",
      "Label: dog, Count: 14\n",
      "Label: door, Count: 10\n",
      "Label: draw, Count: 10\n",
      "Label: dress, Count: 9\n",
      "Label: drink, Count: 21\n",
      "Label: drive, Count: 8\n",
      "Label: drop, Count: 10\n",
      "Label: east, Count: 8\n",
      "Label: easy, Count: 8\n",
      "Label: eat, Count: 9\n",
      "Label: egg, Count: 8\n",
      "Label: enjoy, Count: 10\n",
      "Label: environment, Count: 9\n",
      "Label: example, Count: 8\n",
      "Label: family, Count: 14\n",
      "Label: far, Count: 12\n",
      "Label: fat, Count: 11\n",
      "Label: father, Count: 10\n",
      "Label: fault, Count: 7\n",
      "Label: feel, Count: 9\n",
      "Label: fine, Count: 14\n",
      "Label: finish, Count: 12\n",
      "Label: first, Count: 9\n",
      "Label: fish, Count: 14\n",
      "Label: flower, Count: 9\n",
      "Label: football, Count: 9\n",
      "Label: forget, Count: 14\n",
      "Label: friend, Count: 8\n",
      "Label: friendly, Count: 8\n",
      "Label: full, Count: 12\n",
      "Label: future, Count: 9\n",
      "Label: game, Count: 9\n",
      "Label: girl, Count: 10\n",
      "Label: give, Count: 13\n",
      "Label: glasses, Count: 10\n",
      "Label: go, Count: 17\n",
      "Label: good, Count: 10\n",
      "Label: government, Count: 11\n",
      "Label: graduate, Count: 12\n",
      "Label: green, Count: 10\n",
      "Label: hair, Count: 14\n",
      "Label: halloween, Count: 9\n",
      "Label: happy, Count: 10\n",
      "Label: hard, Count: 7\n",
      "Label: hat, Count: 13\n",
      "Label: have, Count: 8\n",
      "Label: headache, Count: 13\n",
      "Label: hear, Count: 10\n",
      "Label: hearing, Count: 14\n",
      "Label: heart, Count: 9\n",
      "Label: help, Count: 14\n",
      "Label: here, Count: 9\n",
      "Label: home, Count: 9\n",
      "Label: hope, Count: 11\n",
      "Label: hot, Count: 15\n",
      "Label: hour, Count: 9\n",
      "Label: house, Count: 9\n",
      "Label: how, Count: 9\n",
      "Label: humble, Count: 10\n",
      "Label: hurry, Count: 7\n",
      "Label: husband, Count: 9\n",
      "Label: improve, Count: 9\n",
      "Label: inform, Count: 9\n",
      "Label: interest, Count: 12\n",
      "Label: internet, Count: 10\n",
      "Label: jacket, Count: 10\n",
      "Label: join, Count: 10\n",
      "Label: jump, Count: 8\n",
      "Label: kill, Count: 9\n",
      "Label: kiss, Count: 11\n",
      "Label: knife, Count: 6\n",
      "Label: know, Count: 11\n",
      "Label: language, Count: 13\n",
      "Label: last, Count: 12\n",
      "Label: late, Count: 10\n",
      "Label: later, Count: 13\n",
      "Label: laugh, Count: 12\n",
      "Label: law, Count: 10\n",
      "Label: learn, Count: 11\n",
      "Label: leave, Count: 12\n",
      "Label: letter, Count: 12\n",
      "Label: light, Count: 11\n",
      "Label: like, Count: 17\n",
      "Label: list, Count: 12\n",
      "Label: live, Count: 6\n",
      "Label: lose, Count: 12\n",
      "Label: make, Count: 9\n",
      "Label: man, Count: 13\n",
      "Label: many, Count: 13\n",
      "Label: match, Count: 10\n",
      "Label: mean, Count: 8\n",
      "Label: meat, Count: 9\n",
      "Label: medicine, Count: 9\n",
      "Label: meet, Count: 13\n",
      "Label: milk, Count: 9\n",
      "Label: money, Count: 8\n",
      "Label: more, Count: 9\n",
      "Label: most, Count: 10\n",
      "Label: mother, Count: 16\n",
      "Label: movie, Count: 7\n",
      "Label: music, Count: 10\n",
      "Label: name, Count: 11\n",
      "Label: need, Count: 11\n",
      "Label: new, Count: 10\n",
      "Label: no, Count: 15\n",
      "Label: none, Count: 8\n",
      "Label: now, Count: 14\n",
      "Label: office, Count: 10\n",
      "Label: old, Count: 8\n",
      "Label: orange, Count: 14\n",
      "Label: order, Count: 8\n",
      "Label: paint, Count: 11\n",
      "Label: pants, Count: 10\n",
      "Label: paper, Count: 12\n",
      "Label: party, Count: 9\n",
      "Label: past, Count: 10\n",
      "Label: pencil, Count: 7\n",
      "Label: person, Count: 10\n",
      "Label: pink, Count: 12\n",
      "Label: pizza, Count: 15\n",
      "Label: plan, Count: 9\n",
      "Label: play, Count: 13\n",
      "Label: please, Count: 8\n",
      "Label: police, Count: 10\n",
      "Label: practice, Count: 8\n",
      "Label: president, Count: 11\n",
      "Label: problem, Count: 9\n",
      "Label: pull, Count: 12\n",
      "Label: purple, Count: 11\n",
      "Label: rabbit, Count: 11\n",
      "Label: read, Count: 9\n",
      "Label: red, Count: 9\n",
      "Label: remember, Count: 10\n",
      "Label: restaurant, Count: 8\n",
      "Label: ride, Count: 8\n",
      "Label: right, Count: 12\n",
      "Label: room, Count: 10\n",
      "Label: run, Count: 10\n",
      "Label: russia, Count: 11\n",
      "Label: salt, Count: 11\n",
      "Label: same, Count: 12\n",
      "Label: sandwich, Count: 11\n",
      "Label: school, Count: 11\n",
      "Label: secretary, Count: 13\n",
      "Label: share, Count: 10\n",
      "Label: shirt, Count: 14\n",
      "Label: short, Count: 13\n",
      "Label: show, Count: 10\n",
      "Label: sick, Count: 11\n",
      "Label: sign, Count: 9\n",
      "Label: since, Count: 9\n",
      "Label: small, Count: 10\n",
      "Label: snow, Count: 8\n",
      "Label: some, Count: 9\n",
      "Label: son, Count: 12\n",
      "Label: soon, Count: 10\n",
      "Label: south, Count: 10\n",
      "Label: stay, Count: 9\n",
      "Label: student, Count: 11\n",
      "Label: study, Count: 15\n",
      "Label: sunday, Count: 11\n",
      "Label: table, Count: 8\n",
      "Label: take, Count: 11\n",
      "Label: tall, Count: 14\n",
      "Label: tea, Count: 9\n",
      "Label: teach, Count: 10\n",
      "Label: teacher, Count: 12\n",
      "Label: tell, Count: 12\n",
      "Label: test, Count: 8\n",
      "Label: thanksgiving, Count: 15\n",
      "Label: theory, Count: 9\n",
      "Label: thin, Count: 16\n",
      "Label: thursday, Count: 13\n",
      "Label: time, Count: 10\n",
      "Label: tired, Count: 10\n",
      "Label: tomato, Count: 8\n",
      "Label: trade, Count: 11\n",
      "Label: train, Count: 9\n",
      "Label: travel, Count: 10\n",
      "Label: ugly, Count: 11\n",
      "Label: visit, Count: 12\n",
      "Label: wait, Count: 13\n",
      "Label: walk, Count: 11\n",
      "Label: want, Count: 11\n",
      "Label: war, Count: 9\n",
      "Label: water, Count: 12\n",
      "Label: week, Count: 11\n",
      "Label: what, Count: 10\n",
      "Label: where, Count: 9\n",
      "Label: white, Count: 13\n",
      "Label: who, Count: 15\n",
      "Label: why, Count: 8\n",
      "Label: wife, Count: 11\n",
      "Label: window, Count: 8\n",
      "Label: with, Count: 10\n",
      "Label: woman, Count: 13\n",
      "Label: work, Count: 12\n",
      "Label: write, Count: 12\n",
      "Label: wrong, Count: 12\n",
      "Label: year, Count: 13\n",
      "Label: yellow, Count: 11\n",
      "Label: yes, Count: 15\n",
      "Label: yesterday, Count: 12\n",
      "Label: you, Count: 12\n",
      "Label: your, Count: 11\n",
      "Sequence lengths statistics:\n",
      "Min length: 60 Max length: 60 Mean length: 60.0\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 labels:\")\n",
    "print(labels[:5])\n",
    "print(\"First 5 lengths:\")\n",
    "print(lengths[:5])\n",
    "\n",
    "\n",
    "# After data, labels, lengths = load_keypoints(DATA_DIR)\n",
    "print(\"Raw data statistics:\")\n",
    "if data.size > 0:\n",
    "    # Print shapes of the first few samples\n",
    "    print(\"Number of samples:\", len(data))\n",
    "    print(\"Shape of sample 0:\", data[0].shape)\n",
    "    # Compute min, max and mean for the first sample (raw frames)\n",
    "    sample0 = data[0]\n",
    "    print(\"Sample 0 - min:\", np.min(sample0), \"max:\", np.max(sample0), \"mean:\", np.mean(sample0))\n",
    "else:\n",
    "    print(\"No data loaded.\")\n",
    "\n",
    "print(\"Label distribution:\")\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "for l, count in zip(unique, counts):\n",
    "    print(f\"Label: {l}, Count: {count}\")\n",
    "\n",
    "print(\"Sequence lengths statistics:\")\n",
    "print(\"Min length:\", np.min(lengths), \"Max length:\", np.max(lengths), \"Mean length:\", np.mean(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb7f9a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after normalization: (3202, 60, 225)\n"
     ]
    }
   ],
   "source": [
    "# Feature normalization (z-score)\n",
    "def normalize_features(data):\n",
    "    # Flatten all frames for all samples\n",
    "    flat = data.reshape(-1, data.shape[-1])\n",
    "    mean = np.mean(flat, axis=0)\n",
    "    std = np.std(flat, axis=0)\n",
    "    std[std < 1e-10] = 1.0  # avoid division by zero\n",
    "    normalized = (data - mean) / std\n",
    "    return normalized, mean, std\n",
    "\n",
    "data, feat_mean, feat_std = normalize_features(data)\n",
    "print(f\"Data shape after normalization: {data.shape}\")\n",
    "\n",
    "def create_mask(lengths, max_len):\n",
    "    # lengths: (num_samples,)\n",
    "    # returns mask: (num_samples, max_len) with 1 for real, 0 for pad\n",
    "    mask = np.zeros((len(lengths), max_len), dtype=np.float32)\n",
    "    for i, l in enumerate(lengths):\n",
    "        mask[i, :l] = 1.0\n",
    "    return mask\n",
    "\n",
    "mask = create_mask(lengths, data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0d40e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data statistics:\n",
      "Data shape: (3202, 60, 225)\n",
      "Min value: -27.4629 Max value: 25.278162 Mean value: -2.44462e-06 Std Dev: 1.0000072\n"
     ]
    }
   ],
   "source": [
    "# After data, feat_mean, feat_std = normalize_features(data)\n",
    "print(\"Normalized data statistics:\")\n",
    "if data.size > 0:\n",
    "    print(\"Data shape:\", data.shape)\n",
    "    print(\"Min value:\", np.min(data), \"Max value:\", np.max(data), \"Mean value:\", np.mean(data), \"Std Dev:\", np.std(data))\n",
    "else:\n",
    "    print(\"No data available after normalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60166d",
   "metadata": {},
   "source": [
    "# Dataset Preparation: Train-Test Split\n",
    "\n",
    "Split the dataset into training, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split if not already imported\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Create a mapping from string labels to numeric indices\n",
    "label_to_index = {label: idx for idx, label in enumerate(np.unique(labels))}\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "print(f\"Label to index mapping: {label_to_index}\")\n",
    "\n",
    "# Convert string labels to numeric indices\n",
    "numeric_labels = np.array([label_to_index[label] for label in labels])\n",
    "print(f\"Converted labels to numeric indices.\")\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_temp, y_train, y_temp, mask_train, mask_temp = train_test_split(data, numeric_labels, mask, test_size=0.3, stratify=numeric_labels, random_state=42)\n",
    "X_val, X_test, y_val, y_test, mask_val, mask_test = train_test_split(X_temp, y_temp, mask_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e866d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class ASLDataset(Dataset):\n",
    "    def __init__(self, data, labels, mask):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx], self.mask[idx]\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = ASLDataset(X_train, y_train, mask_train)\n",
    "val_dataset = ASLDataset(X_val, y_val, mask_val)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb276e",
   "metadata": {},
   "source": [
    "# Model Definition: BiLSTM for Temporal Data\n",
    "\n",
    "Define a BiLSTM model for ASL recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95231d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Print information about the data shape\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "if len(data) > 0:\n",
    "    print(f\"Each sample has {data[0].shape[0]} frames with {data[0].shape[1]} features per frame\")\n",
    "\n",
    "# Replace RNNModel with BiLSTM\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout_rate=0.3):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout_rate if num_layers > 1 else 0\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (batch, seq, feat)\n",
    "        # mask: (batch, seq)\n",
    "        out, _ = self.lstm(x)  # (batch, seq, hidden*2)\n",
    "        if mask is not None:\n",
    "            # For each sample, get the last valid (unpadded) output\n",
    "            lengths = mask.sum(dim=1).long()  # (batch,)\n",
    "            last_outputs = []\n",
    "            for i, l in enumerate(lengths):\n",
    "                last_outputs.append(out[i, l-1, :])\n",
    "            out = torch.stack(last_outputs, dim=0)  # (batch, hidden*2)\n",
    "        else:\n",
    "            out = out[:, -1, :]\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Get the input size from the processed data\n",
    "input_size = data[0].shape[1] if len(data) > 0 else 0\n",
    "hidden_size = 128\n",
    "output_size = len(np.unique(labels))\n",
    "print(f\"Input size for the model: {input_size}\")\n",
    "print(f\"Number of unique labels (classes): {output_size}\")\n",
    "\n",
    "model = BiLSTM(input_size, hidden_size, output_size)\n",
    "print(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9840ccc",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Grid Search\n",
    "\n",
    "We use a custom grid search implementation for hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the grid search implementation\n",
    "from random_search_implementation import grid_search_hyperparameter_optimization\n",
    "import numpy as np\n",
    "\n",
    "print(\"Starting hyperparameter optimization using Grid Search...\")\n",
    "\n",
    "# Define the hyperparameter grid (choose reasonable values for your problem)\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-3, 5e-3, 1e-2],\n",
    "    'hidden_size': [32, 64, 128],\n",
    "    'dropout_rate': [0.2, 0.4, 0.6, 0.8],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'weight_decay': [1e-4, 1e-3],\n",
    "    'num_layers': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Run grid search for hyperparameter optimization\n",
    "best_params, results = grid_search_hyperparameter_optimization(\n",
    "    model_class=BiLSTM,\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    device=device,\n",
    "    param_grid=param_grid,\n",
    "    max_time_seconds=3600  # Run for max of 1 hour\n",
    ")\n",
    "\n",
    "# Extract best validation loss\n",
    "best_val_loss = min(r['val_loss'] for r in results)\n",
    "\n",
    "# Print optimization results\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Value (validation loss): {best_val_loss:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Ensure all required parameters are stored for model training\n",
    "best_params = {\n",
    "    'learning_rate': best_params['learning_rate'],\n",
    "    'hidden_size': best_params['hidden_size'],\n",
    "    'dropout_rate': best_params['dropout_rate'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'weight_decay': best_params['weight_decay'],\n",
    "    'num_layers': best_params['num_layers']\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17410aea",
   "metadata": {},
   "source": [
    "# Model Training with K-Fold Cross-Validation\n",
    "\n",
    "Train the model using k-fold cross-validation to better evaluate its performance. This section also switches to an RNN-based architecture and ensures proper batch normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b730f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.notebook import tqdm  # Use notebook-friendly version of tqdm\n",
    "\n",
    "# Define the RNN model for classification\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout_rate=0.3):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size, hidden_size, batch_first=True, num_layers=num_layers, dropout=dropout_rate if num_layers > 1 else 0\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        # Dropout after RNN output\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.layer_norm(x[:, -1, :])\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Use best hyperparameters from grid search\n",
    "best_lr = best_params['learning_rate']\n",
    "best_hidden_size = best_params['hidden_size']\n",
    "best_dropout_rate = best_params['dropout_rate']\n",
    "best_batch_size = max(2, best_params['batch_size'])  # Ensure batch size is above 1\n",
    "best_weight_decay = best_params['weight_decay']\n",
    "best_num_layers = best_params['num_layers']\n",
    "\n",
    "print(f\"Training with best hyperparameters:\")\n",
    "print(f\"  Learning rate: {best_lr}\")\n",
    "print(f\"  Hidden size: {best_hidden_size}\")\n",
    "print(f\"  Dropout rate: {best_dropout_rate}\")\n",
    "print(f\"  Batch size: {best_batch_size}\")\n",
    "print(f\"  Weight decay: {best_weight_decay}\")\n",
    "print(f\"  Number of layers: {best_num_layers}\")\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "k_folds = 3\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Training with k-fold cross-validation\n",
    "fold_results = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
    "    print(f\"Fold {fold+1}/{k_folds}\")\n",
    "\n",
    "    # Create data loaders for the current fold\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    val_subset = Subset(train_dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=best_batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "    # Recreate model for each fold with best hyperparameters\n",
    "    model = BiLSTM(\n",
    "        input_size, best_hidden_size, output_size, num_layers=best_num_layers, dropout_rate=best_dropout_rate).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer with weight decay\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n",
    "    # Use ReduceLROnPlateau scheduler for better learning rate adaptation\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    patience = 10\n",
    "    early_stop_counter = 0\n",
    "    best_val_loss = float('inf')\n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\", end=\"\\r\")\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training\", leave=False):\n",
    "            inputs, targets, mask = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, mask)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Validation\", leave=False):\n",
    "                inputs, targets, mask = batch\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs, mask)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Print progress only once per epoch\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save the model if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"best_model_fold{fold+1}.pth\")\n",
    "            print(\"Model checkpoint saved!\")\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"Early stopping counter: {early_stop_counter}/{patience}\")\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    fold_results.append(best_val_loss)\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"\\nCross-validation results:\")\n",
    "for fold, loss in enumerate(fold_results):\n",
    "    print(f\"Fold {fold+1}: Validation Loss = {loss:.4f}\")\n",
    "print(f\"Average Validation Loss: {sum(fold_results)/len(fold_results):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Loss Curves\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curves\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7bf6d",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Evaluate the model on the test set and display metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_dataset = ASLDataset(X_test, y_test, mask_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, targets, mask = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move inputs to device too\n",
    "        outputs = model(inputs, mask)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "test_loss /= len(test_loader)\n",
    "accuracy = correct / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the model for transfer learning\n",
    "os.makedirs('./models', exist_ok=True)  # Create directory if it doesn't exist\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'input_size': input_size,\n",
    "    'hidden_size': best_hidden_size,\n",
    "    'dropout_rate': best_dropout_rate,\n",
    "    'output_size': output_size,\n",
    "    'accuracy': accuracy\n",
    "}, \"./models/transfer_learning_model.pth\")\n",
    "print(\"Model saved for transfer learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90776ea",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Test the model on new data or perform real-time inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb18cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "model.eval()\n",
    "sample = torch.tensor(X_test[0:1], dtype=torch.float32).to(device)  # Move sample to the correct device\n",
    "with torch.no_grad():\n",
    "    prediction = model(sample)\n",
    "    predicted_label = torch.argmax(prediction, dim=1).item()\n",
    "print(f\"Predicted label: {predicted_label}, True label: {y_test[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aslpose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
